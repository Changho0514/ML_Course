{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Changho0514/web1/blob/main/file3_PretrainBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXkc_eQ16ifL",
        "outputId": "02c08d3c-6a18-488b-e82c-72aed2753204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.11.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2zu3Lo_d7f4U"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE # byte-pair encoding\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jUI21EeU7f4Z"
      },
      "outputs": [],
      "source": [
        "# file 1에서 정의한 dataset class\n",
        "class TokenIndexer():\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __call__(self, tokens):\n",
        "        return [self.tokenizer.token_to_id(token) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        return [self.tokenizer.id_to_token(id) for id in ids]\n",
        "\n",
        "\n",
        "class SentencePairDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # hyper parameters\n",
        "        self.max_len = 128\n",
        "        self.mask_prob = 0.15\n",
        "        self.max_pred = 20 \n",
        "\n",
        "        labels = (\"0\", \"1\")\n",
        "\n",
        "        # data 다운로드 및 전처리\n",
        "        dataset = torchtext.datasets.WikiText2(root='.data', split='train')\n",
        "        dataset = [d.strip() for d in dataset if len(d)>10] # 길이가 짧아 무의미한 데이터는 제외하고, strip을 통해 데이터를 정리해주겠습니다.\n",
        "        \n",
        "        \n",
        "        # data 처리\n",
        "        nlp = English()\n",
        "        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "        doc = nlp(dataset[0])\n",
        "        self.data=[]\n",
        "        self.data_length = 0\n",
        "        self.data_index = {}\n",
        "        self.sentence_lengths = []\n",
        "        for i,context in enumerate(tqdm(dataset)):\n",
        "            doc = nlp(context)\n",
        "            sentences = [sent.string.strip() for sent in doc.sents]\n",
        "            if len(sentences)<=3: # can not obtain positive, negative pair in dataset\n",
        "                continue\n",
        "            self.data.append(sentences)\n",
        "            for j in range(len(sentences)-1): # drop last sentence (no positive pair)\n",
        "                self.data_index[j+self.data_length] = (len(self.data)-1,j)\n",
        "            self.data_length += len(sentences)-1\n",
        "            self.sentence_lengths.append(len(sentences))\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        # tokenizer\n",
        "        save_data = '\\n'.join(['\\n'.join(d) for d in self.data])\n",
        "        with open('data.txt','w') as f:\n",
        "          f.write(save_data)\n",
        "        self.tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "        trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"<unk>\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "        files = ['data.txt']\n",
        "        self.tokenizer.train(files, trainer)\n",
        "        self.vocab_words = list(self.tokenizer.get_vocab().keys())\n",
        "        self.token_indexer = TokenIndexer(self.tokenizer)\n",
        "\n",
        "\n",
        "    def get_negative_sample(self, index):\n",
        "        i, j = self.data_index[index]\n",
        "        max_j = self.sentence_lengths[i]\n",
        "        while True:\n",
        "            random_j = np.random.randint(0,max_j)\n",
        "            if random_j != (j+1) and random_j != j :\n",
        "                break\n",
        "        i, new_j = self.data_index[index+(random_j-j)]\n",
        "        return self.data[i][new_j]\n",
        "\n",
        "    def get_positive_sample(self, index):\n",
        "        i, j = self.data_index[index]\n",
        "        return self.data[i][j+1]\n",
        "    \n",
        "    \n",
        "    def process_instance(self, instance):\n",
        "        # -3  for special tokens [CLS], [SEP], [SEP]\n",
        "        is_next, tokens_a, tokens_b = instance\n",
        "        while len(tokens_a)+len(tokens_b) > self.max_len-3:\n",
        "            if len(tokens_a)>len(tokens_b) : \n",
        "                tokens_a = tokens_a[:-1]\n",
        "            else:\n",
        "                tokens_b = tokens_b[:-1]\n",
        "        \n",
        "        tokens = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
        "        segment_ids = [0]*(len(tokens_a)+2) + [1]*(len(tokens_b)+1)\n",
        "        input_mask = [1]*len(tokens)\n",
        "        \n",
        "        #masked LM\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        n_pred = min(self.max_pred, max(1, int(round(len(tokens)*self.mask_prob))))\n",
        "\n",
        "\n",
        "        candidate_position = [i for i, token in enumerate(tokens) if token != '[CLS]' and token != '[SEP]']\n",
        "        np.random.shuffle(candidate_position)\n",
        "\n",
        "        for pos in candidate_position[:n_pred]:\n",
        "            masked_tokens.append(tokens[pos])\n",
        "            masked_pos.append(pos)\n",
        "            if np.random.random() < 0.8: # 80%\n",
        "                tokens[pos] = '[MASK]'\n",
        "            elif np.random.random() < 0.5: # 10%\n",
        "                random_word = self.vocab_words[np.random.randint(0, len(self.vocab_words)-1)]\n",
        "                tokens[pos] = random_word\n",
        "\n",
        "        # when n_pred < max_pred, we only calculate loss within n_pred\n",
        "        masked_weights = [1]*len(masked_tokens)\n",
        "\n",
        "        # Token Indexing\n",
        "        input_ids = self.token_indexer(tokens)\n",
        "        masked_ids = self.token_indexer(masked_tokens)\n",
        "\n",
        "        # Zero Padding\n",
        "        n_pad = self.max_len - len(input_ids)\n",
        "        input_ids.extend([0]*n_pad)\n",
        "        segment_ids.extend([0]*n_pad)\n",
        "        input_mask.extend([0]*n_pad)\n",
        "\n",
        "        # Zero Padding for masked target\n",
        "        if self.max_pred > n_pred:\n",
        "            n_pad = self.max_pred - n_pred\n",
        "            masked_ids.extend([0]*n_pad)\n",
        "            masked_pos.extend([0]*n_pad)\n",
        "            masked_weights.extend([0]*n_pad)\n",
        "        \n",
        "        \n",
        "        return (input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next)\n",
        "    \n",
        "    \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i,j = self.data_index[index]\n",
        "        sentence_a = self.data[i][j]\n",
        "        token_a = self.tokenizer.encode(sentence_a).tokens\n",
        "        \n",
        "        is_next = np.random.random()\n",
        "        if is_next < 0.5:\n",
        "            is_next = 0\n",
        "            sentence_b = self.get_negative_sample(index)\n",
        "        else:\n",
        "            is_next = 1\n",
        "            sentence_b = self.get_positive_sample(index)\n",
        "        token_b = self.tokenizer.encode(sentence_b).tokens\n",
        "            \n",
        "        instance = (is_next, token_a, token_b)\n",
        "        input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next = self.process_instance(instance)\n",
        "        \n",
        "        input_ids = torch.tensor(input_ids).long()\n",
        "        segment_ids = torch.tensor(segment_ids).long()\n",
        "        input_mask = torch.tensor(input_mask).long()\n",
        "        masked_ids = torch.tensor(masked_ids).long()\n",
        "        masked_pos = torch.tensor(masked_pos).long()\n",
        "        masked_weights = torch.tensor(masked_weights).long()\n",
        "        is_next = torch.tensor(is_next).long()\n",
        "        \n",
        "        \n",
        "        return input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "3ce1ffdbe8b54961b8a74b129f693141",
            "17d1f140b8bf4f6780ade36d275dcf87",
            "6394afbf96b14ff09792fd242aaa9592",
            "49518e12bffa400cb3caeac334f530f6",
            "7f94d31585984b8c8afba77fe433d5fe",
            "71f0e0b1827440c08ae5d5f9e56e70b6",
            "c2a28d52f7ad410f84e70717fc91978d",
            "b9a90f9c9c7b4ab382a5ddd652e8b31a",
            "338706f22f334678b750aef975b332ae",
            "b46cd44ab0644324ac5183d81a2598c0",
            "36e760db347e4be8a27c6753623afa79"
          ]
        },
        "id": "pzLyyhiQ7f4b",
        "outputId": "19b04fca-7785-4200-c357-562c2ae1fe5a",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:00<00:00, 6.60MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ce1ffdbe8b54961b8a74b129f693141",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/23627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = SentencePairDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sFqgNFS3-lXV"
      },
      "outputs": [],
      "source": [
        "# file 2에서 정의한 모델\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get mean, variance\n",
        "        u = x.mean(-1, keepdim=True) # 각 단어별 mean\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True) # 각 단어별 variance\n",
        "        \n",
        "        # normalize\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # (x - mean)/std \n",
        "        \n",
        "        return self.gamma * x + self.beta # gamma, beta를 이용해 mean, std 조정\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.dim) # token embedding\n",
        "        self.pos_embed = nn.Embedding(cfg.max_len, cfg.dim) # position embedding\n",
        "        self.seg_embed = nn.Embedding(cfg.n_segments, cfg.dim) # segment(token type) embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device) # 0,1,2,3,4,5, ..., seq_len-1\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.drop(self.norm(e))\n",
        "class Attention(nn.Module):\n",
        "    #Scaled Dot Product Attention\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "                 / math.sqrt(query.size(-1)) # scale\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        B,S,D = x.shape\n",
        "\n",
        "\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        \n",
        "        q, k, v = (x.reshape(B,S,self.n_heads,-1).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1)) # @ == torch.matmul (dot product)\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :].float()\n",
        "            scores -= 10000.0 * (1.0 - mask)\n",
        "        scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = (scores @ v).transpose(1, 2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = h.reshape(B,S,D)\n",
        "        self.scores = scores\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
        "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadedSelfAttention(cfg)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.attn(x, mask)\n",
        "        h = self.norm1(x + self.drop(self.proj(h)))\n",
        "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "        return h\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.embed = Embeddings(cfg)\n",
        "        self.blocks = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "    def forward(self, x, seg, mask):\n",
        "        h = self.embed(x, seg)\n",
        "        for block in self.blocks:\n",
        "            h = block(h, mask)\n",
        "        return h\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cE7AGtkp7f4g"
      },
      "outputs": [],
      "source": [
        "# pretrain 을 위해 정의한 BERT 모델\n",
        "class BERT4Pretrain(nn.Module):\n",
        "    \"Bert Model for Pretrain : Masked LM and next sentence classification\"\n",
        "    def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "        self.transformer = Transformer(cfg)\n",
        "        self.fc = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.activ1 = nn.Tanh()\n",
        "        self.linear = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.activ2 = gelu\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        \n",
        "        # next sentence prediction classifier\n",
        "        self.classifier = nn.Linear(cfg.dim, 2)\n",
        "        \n",
        "        # word classification layer\n",
        "\n",
        "\n",
        "        # decoder is shared with embedding layer\n",
        "\n",
        "        # word embedding layer parameter 그대로 사용 \n",
        "        embed_weight = self.transformer.embed.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)  # vector dim을 \n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, input_mask, masked_pos):\n",
        "        # transformer(encoder) 통과\n",
        "        h = self.transformer(input_ids, segment_ids, input_mask)\n",
        "\n",
        "        # mask된 곳들에 대해서만 language modeling 한다. \n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, h.size(-1)) # mask인 곳만 1, 나머지는 0 [B, S, F]\n",
        "        h_masked = torch.gather(h, 1, masked_pos)  # mask가 1인 곳에 있는 값만 가져옴\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked))) # linear -> activation -> normalization\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # wrod classification layer\n",
        "        \n",
        "        # NSP\n",
        "        pooled_h = self.activ1(self.fc(h[:,0])) # liinear + activation [B, F]\n",
        "        logits_clsf = self.classifier(pooled_h) # [CLS] -> NSP classification 전용\n",
        "\n",
        "        \n",
        "\n",
        "        return logits_lm, logits_clsf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qirzru3e7f4j"
      },
      "outputs": [],
      "source": [
        "sample_config = {\n",
        "    \"dim\": 768,\n",
        "    \"dim_ff\": 3072,\n",
        "    \"n_layers\": 12,\n",
        "    \"p_drop_attn\": 0.1,\n",
        "    \"n_heads\": 12,\n",
        "    \"p_drop_hidden\": 0.1,\n",
        "    \"max_len\": 512,\n",
        "    \"n_segments\": 2,\n",
        "    \"vocab_size\": 30522\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "model_config = AttributeDict(sample_config)\n",
        "\n",
        "\n",
        "\n",
        "model = BERT4Pretrain(model_config)\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMgpXz3i7f4l",
        "outputId": "f1be2cf2-420a-459d-ab9e-2c97a1c82cd6",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERT4Pretrain(\n",
              "  (transformer): Transformer(\n",
              "    (embed): Embeddings(\n",
              "      (tok_embed): Embedding(30522, 768)\n",
              "      (pos_embed): Embedding(512, 768)\n",
              "      (seg_embed): Embedding(2, 768)\n",
              "      (norm): LayerNorm()\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): Block(\n",
              "        (attn): MultiHeadedSelfAttention(\n",
              "          (proj_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (proj_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (norm1): LayerNorm()\n",
              "        (pwff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm()\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (activ1): Tanh()\n",
              "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (norm): LayerNorm()\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cW94oPC57f4o"
      },
      "outputs": [],
      "source": [
        "# 학습용 hyperparameters\n",
        "train_config = AttributeDict({\n",
        "    \"batch_size\": 16,\n",
        "    \"lr\": 1e-4,\n",
        "    \"n_epochs\": 5,\n",
        "    \"warmup\": 0.1,\n",
        "    \"total_steps\": 1000\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n04F8eEg7f4r"
      },
      "outputs": [],
      "source": [
        "# loss 와 optimize 기법 정의\n",
        "criterion1 = nn.CrossEntropyLoss(reduction='none')\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def get_loss(model, batch): \n",
        "    input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next = batch\n",
        "    '''\n",
        "    input_ids : token index들 (1 문장 3 문장 3 0 0 0 0 0)\n",
        "    segment_ids : 앞문장 뒷문장 구분 (0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0)\n",
        "    input_mask : 입력과 패딩을 구분 ( 1 1 1 1 1 1 1 1  0 0 0 0 0 )\n",
        "    masked_ids : MLM을 위해 [MASK]로 바꾼 애들 (random word, 가만히 유지)의 원래 token index\n",
        "    masked_pos : [MASK]로 바꾼 애들의 문장 내 위치 [3 15 32]\n",
        "    masked_weights : [MASK]로 바꾼 것 만큼의 1[1 1 1  0 0 0 0]\n",
        "    is_next : next sentance prediction label\n",
        "    '''\n",
        "\n",
        "    logits_lm, logits_clsf = model(input_ids, segment_ids, input_mask, masked_pos)\n",
        "\n",
        "    # MLM\n",
        "    # logits_lm : [B, mask개수, class 개수(vocab size)]-> [B, class개수(vocab size), mask 개수]\n",
        "    # masked_ids: [B, mask개수]\n",
        "    loss_lm = criterion1(logits_lm.transpose(1, 2), masked_ids) # for masked LM\n",
        "    \n",
        "    loss_lm = (loss_lm*masked_weights.float()).mean()\n",
        "    loss_clsf = criterion2(logits_clsf, is_next) # for sentence classification\n",
        "    return loss_lm + loss_clsf\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), train_config['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AK-2koCH7f4t"
      },
      "outputs": [],
      "source": [
        "class Sequential_indices_Sampler(Sampler):\n",
        "    def __init__(self, data_source):\n",
        "        self.data_source = data_source\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.data_source)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_source)\n",
        "\n",
        "\n",
        "# small dataset\n",
        "train_indices = range(0,2000,1) # 2000개 데이터로 학습\n",
        "val_indices = range(2000,2100,1) # 100개 데이터로 평가\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = Sequential_indices_Sampler(val_indices)\n",
        "\n",
        "train_loader = data.DataLoader(dataset, batch_size=train_config.batch_size, sampler=train_sampler)\n",
        "val_loader = data.DataLoader(dataset, batch_size=train_config.batch_size, sampler=val_sampler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430,
          "referenced_widgets": [
            "8304ca99aa3549b29fb5019675b2efbc",
            "51e045d9617540bba77e4acaae0cc9b5",
            "98214b4454bb429c897fd80c2d6fa5b2",
            "9181b78f53324a8bb996e423063f7c1d",
            "66602bb49d5049f69722fedba458c9fd",
            "2bc00dca0d8242e08c2784a93400612f",
            "eec9a48453aa4391943b5c3d5e3779cc",
            "da36213c58f74c1a804deb89a3952c41",
            "d6bcaa1006284febb51c52baffecf82e",
            "f52dc95962ca4da99282ac5138873e23",
            "91cc8246e93e4c5a9c39a6a8efcf89f4"
          ]
        },
        "id": "WJKglCAa7f4w",
        "outputId": "c0249de7-7af0-40d3-f426-ecdbc70089f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8304ca99aa3549b29fb5019675b2efbc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-aa28d9575564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-ae3b7447d784>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(model, batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m     '''\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlogits_lm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_clsf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# MLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c4b7f02a19bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, segment_ids, input_mask, masked_pos)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# transformer(encoder) 통과\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# mask된 곳들에 대해서만 language modeling 한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b391cb19b32c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seg, mask)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b391cb19b32c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seg)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (S,) -> (B, S)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
          ]
        }
      ],
      "source": [
        "# train mode\n",
        "model.train() \n",
        "\n",
        "for epoch in range(train_config.n_epochs):\n",
        "    loss_sum = 0\n",
        "    for i, batch in enumerate(tqdm(train_loader)):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = get_loss(model, batch).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        loss_sum += loss.item()\n",
        "        \n",
        "    print('Epoch {}/{} : Average Loss {:.3f}'.format(epoch+1, train_config.n_epochs, loss_sum/(i+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGiMpmdR7f4y"
      },
      "outputs": [],
      "source": [
        "# validation mode\n",
        "model.eval() \n",
        "\n",
        "with torch.no_grad():\n",
        "    loss_sum = 0\n",
        "    for i, batch in enumerate(tqdm(train_loader)):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        loss = get_loss(model, batch).mean()\n",
        "        loss_sum += loss.item()\n",
        "        \n",
        "    print('Validation Average Loss {:.3f}'.format(loss_sum/(i+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_63Jq007f40"
      },
      "outputs": [],
      "source": [
        "input_ids, segment_ids, input_mask, masked_ids, masked_pos, masked_weights, is_next = batch\n",
        "logits_lm, logits_clsf = model(input_ids, segment_ids, input_mask, masked_pos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbwBcZGr7f43"
      },
      "outputs": [],
      "source": [
        "logits_lm[0].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxgC8YmU7f46"
      },
      "outputs": [],
      "source": [
        "example_idx = 12\n",
        "word2idx = dataset.tokenizer.get_vocab()\n",
        "idx2word = {word2idx[word]:word for word in word2idx}\n",
        "\n",
        "def get_sentence(logits, GT=True):\n",
        "    if not GT:\n",
        "        idxs = logits.argmax(1).tolist()\n",
        "    else:\n",
        "        idxs = logits.tolist()\n",
        "    words = [idx2word[idx] for idx in idxs if idx2word[idx]!='[PAD]']\n",
        "    return words\n",
        "\n",
        "print('#  model input\\n')\n",
        "print(' '.join(get_sentence(input_ids[example_idx])), '\\n')\n",
        "\n",
        "print('\\n\\n')\n",
        "print('#  model prediction at masked position\\n')\n",
        "print(' '.join(get_sentence(logits_lm[example_idx], GT=False)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClXNl6-P7f49"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "file3_PretrainBERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ce1ffdbe8b54961b8a74b129f693141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_17d1f140b8bf4f6780ade36d275dcf87",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6394afbf96b14ff09792fd242aaa9592",
              "IPY_MODEL_49518e12bffa400cb3caeac334f530f6",
              "IPY_MODEL_7f94d31585984b8c8afba77fe433d5fe"
            ]
          }
        },
        "17d1f140b8bf4f6780ade36d275dcf87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6394afbf96b14ff09792fd242aaa9592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_71f0e0b1827440c08ae5d5f9e56e70b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2a28d52f7ad410f84e70717fc91978d"
          }
        },
        "49518e12bffa400cb3caeac334f530f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b9a90f9c9c7b4ab382a5ddd652e8b31a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 23627,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 23627,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_338706f22f334678b750aef975b332ae"
          }
        },
        "7f94d31585984b8c8afba77fe433d5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b46cd44ab0644324ac5183d81a2598c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 23627/23627 [00:13&lt;00:00, 1830.24it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36e760db347e4be8a27c6753623afa79"
          }
        },
        "71f0e0b1827440c08ae5d5f9e56e70b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2a28d52f7ad410f84e70717fc91978d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9a90f9c9c7b4ab382a5ddd652e8b31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "338706f22f334678b750aef975b332ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b46cd44ab0644324ac5183d81a2598c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36e760db347e4be8a27c6753623afa79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8304ca99aa3549b29fb5019675b2efbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_51e045d9617540bba77e4acaae0cc9b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_98214b4454bb429c897fd80c2d6fa5b2",
              "IPY_MODEL_9181b78f53324a8bb996e423063f7c1d",
              "IPY_MODEL_66602bb49d5049f69722fedba458c9fd"
            ]
          }
        },
        "51e045d9617540bba77e4acaae0cc9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98214b4454bb429c897fd80c2d6fa5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2bc00dca0d8242e08c2784a93400612f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eec9a48453aa4391943b5c3d5e3779cc"
          }
        },
        "9181b78f53324a8bb996e423063f7c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_da36213c58f74c1a804deb89a3952c41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 125,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6bcaa1006284febb51c52baffecf82e"
          }
        },
        "66602bb49d5049f69722fedba458c9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f52dc95962ca4da99282ac5138873e23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/125 [00:11&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91cc8246e93e4c5a9c39a6a8efcf89f4"
          }
        },
        "2bc00dca0d8242e08c2784a93400612f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eec9a48453aa4391943b5c3d5e3779cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da36213c58f74c1a804deb89a3952c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6bcaa1006284febb51c52baffecf82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f52dc95962ca4da99282ac5138873e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91cc8246e93e4c5a9c39a6a8efcf89f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}