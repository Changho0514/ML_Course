{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "file5_Translation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Changho0514/web1/blob/main/file5_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcFyZ5vetQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2277b277-546b-43d7-8c88-309854356f82"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "# 수학 관련 라이브러리\n",
        "import numpy as np\n",
        "import math\n",
        "# pytorch 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import tqdm\n",
        "from torch.utils import data # dataset 관련된 utility 를 사용하려는 용도\n",
        "from random import choice, randrange # random\n",
        "from itertools import zip_longest \n",
        "import json \n",
        "import random\n",
        "import pdb\n",
        "\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ABHpkPXsV-Gx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ywlO3lOjcJ5a",
        "outputId": "fb352b01-de98-4d33-ed90-5c0db640f821",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip data_files.zip"
      ],
      "metadata": {
        "id": "R-eRAdsqWanC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYCLJa4petQg"
      },
      "source": [
        "def split_last(x, shape):\n",
        "    \"split the last dimension to given shape\"\n",
        "    shape = list(shape)\n",
        "    assert shape.count(-1) <= 1\n",
        "    if -1 in shape:\n",
        "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
        "    return x.view(*x.size()[:-1], *shape)\n",
        "\n",
        "def merge_last(x, n_dims):\n",
        "    \"merge the last n_dims to a dimension\"\n",
        "    s = x.size()\n",
        "    assert n_dims > 1 and n_dims < len(s)\n",
        "    return x.view(*s[:-n_dims], -1)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQR9lBMetQh"
      },
      "source": [
        "# Activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295zs7CWetQj"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWS8voZZetQk"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUYgHC0ietQk"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get mean, variance\n",
        "        u = x.mean(-1, keepdim=True) # sequence 방향 mean\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True) # sequence 방향 variance\n",
        "        \n",
        "        # normalize\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # (x - mean)/std \n",
        "        \n",
        "        return self.gamma * x + self.beta # gamma, beta를 이용해 mean, std 조정"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMiPES9getQl"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbTPl6aSetQm"
      },
      "source": [
        "def get_sinusoid_encoding_table(n_position, d_model):\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg, vocab_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tok_embed = nn.Embedding(vocab_size, cfg.dim) # token embedding\n",
        "        self.pos_embed = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(512, cfg.dim),freeze=True) # position embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device) # 0,1,2,3,4,5, ..., seq_len-1\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos)\n",
        "        return self.drop(self.norm(e))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsNAdbnetQn"
      },
      "source": [
        "#  Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-ulMozSetQn"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    #Scaled Dot Product Attention\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1)) # scale\n",
        "        \n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDtvhua3etQo"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    def forward(self, x, mask, x_q=None):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        \n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        if x_q is None:\n",
        "            q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        else:\n",
        "            q, k, v = self.proj_q(x_q), self.proj_k(x), self.proj_v(x)\n",
        "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(k.size(-1)) # @ == torch.matmul (dot product)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "            scores = scores.masked_fill_(mask, -1e9)\n",
        "        scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = torch.matmul(scores, v).transpose(1,2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = merge_last(h, 2)\n",
        "        self.scores = scores\n",
        "        return h"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADQUnmexetQo"
      },
      "source": [
        "# Base feedforward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeMI83XbetQo"
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
        "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgFSAOGNetQp"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dEoqWUFetQp"
      },
      "source": [
        "class Encoder_Block(nn.Module):\n",
        "    \"\"\" Transformer Block \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(cfg)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.attn(x, mask)\n",
        "        h = self.norm1(x + self.drop(self.proj(h)))\n",
        "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "        return h\n",
        "    \n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "    \n",
        "def get_attn_subsequent_mask(seq):\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    subsequent_mask = torch.tensor(subsequent_mask, device=seq.device).byte()\n",
        "    return subsequent_mask\n",
        "    \n",
        "    \n",
        "class Decoder_Block(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(cfg)\n",
        "        self.encoder_attention = MultiHeadAttention(cfg)\n",
        "        \n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.proj1 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.proj2 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        \n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm3 = LayerNorm(cfg)\n",
        "        \n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "        \n",
        "    def forward(self,x , enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \n",
        "        # self-attention -> add&norm\n",
        "        h = self.self_attention(x, dec_self_attn_mask)\n",
        "        h = self.norm1(x + self.drop(self.proj1(h)))\n",
        "        \n",
        "        # encoder attention -> add&norm\n",
        "        h2 = self.encoder_attention(enc_outputs, dec_enc_attn_mask, x_q=h)\n",
        "        h = self.norm2(h + self.drop(self.proj2(h2))) \n",
        "        \n",
        "        # feedforward network\n",
        "        h = self.norm3(h + self.drop(self.pwff(h)))\n",
        "        \n",
        "        return h\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        #====================encoder===========================\n",
        "        self.encoder_embed = Embeddings(cfg, len(korean_vocab))\n",
        "        self.encoder_blocks = nn.ModuleList([Encoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        #====================decoder============================\n",
        "        self.decoder_embed = Embeddings(cfg, len(english_vocab))\n",
        "        self.decoder_blocks = nn.ModuleList([Decoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "        \n",
        "        #=========================================================\n",
        "        self.projection = nn.Linear(cfg.dim, len(english_vocab))\n",
        "        \n",
        "        \n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        #============encoder============\n",
        "        h = self.encoder_embed(enc_inputs)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "        for block in self.encoder_blocks:\n",
        "            h = block(h, enc_self_attn_mask)\n",
        "            \n",
        "        enc_outputs = h\n",
        "        \n",
        "        \n",
        "        #============decoder============\n",
        "        \n",
        "        # self attention mask\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).float()\n",
        "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs).float()\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "\n",
        "        # encoder attention mask\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "        \n",
        "        \n",
        "        # embedding\n",
        "        h = self.decoder_embed(dec_inputs)\n",
        "        \n",
        "        \n",
        "        for block in self.decoder_blocks:\n",
        "            h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "        #============projection==========\n",
        "        \n",
        "        out = self.projection(h)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def greedy_decoding(self, enc_inputs, start_token_index = 1, end_token_index = 2, generation_max_len=128):\n",
        "        # encoder, decoder 따로 넣어줘야함. 왜? => 나중에 따로 쓰려고.\n",
        "        # encoding 한번 하면 그 값이 바뀌지 않음. \n",
        "        with torch.no_grad():\n",
        "            batch_size, max_length = enc_inputs.size()\n",
        "            generation_end_flag = [0 for i in range(batch_size)]\n",
        "            predicted_sentences = []\n",
        "            #=================encoding=============\n",
        "            h = self.encoder_embed(enc_inputs)\n",
        "            enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "            for block in self.encoder_blocks:\n",
        "                h = block(h, enc_self_attn_mask)    \n",
        "            enc_outputs = h\n",
        "\n",
        "            #================ greedy decoding ==================\n",
        "            # dec_inputs : (batch size, 1) # <SOS> 토큰이 들어가야함.\n",
        "            dec_inputs = torch.ones(batch_size, 1, device=enc_inputs.device) * start_token_index\n",
        "            dec_inputs = dec_inputs.long() # long type으로\n",
        "\n",
        "            for i in range(generation_max_len):\n",
        "\n",
        "                #====================== decoder =======================\n",
        "                # self attention mask\n",
        "                dec_self_attn_pad_mask = None\n",
        "                dec_self_attn_subsequent_mask = None\n",
        "                dec_self_attn_mask = None\n",
        "\n",
        "                # encoder attention mask\n",
        "                dec_enc_attn_mask = None\n",
        "\n",
        "\n",
        "                # embedding\n",
        "                h = self.decoder_embed(dec_inputs)\n",
        "\n",
        "\n",
        "                for block in self.decoder_blocks:\n",
        "                    h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "\n",
        "\n",
        "                out = self.projection(h[:,-1,:])  # -1 왜? : [B, S, F] 에서 마지막 새 단어를 만들어주기 위해서 마지막 단어만 projection\n",
        "                pred = out.argmax(-1) # 왜? : 가장 확률이 높은 인덱스를 반환하기 위해서\n",
        "\n",
        "#                 print(out.size(), pred)\n",
        "\n",
        "                dec_inputs = torch.cat((dec_inputs, pred.unsqueeze(1)),dim=1) # [B, 1]-> [B, 2]-> [B, 3]...\n",
        "    #             print(dec_inputs)\n",
        "\n",
        "                predicted_sentences.append(pred)\n",
        "                for j, boolean in enumerate(pred==end_token_index):\n",
        "                    if boolean == True:\n",
        "                        generation_end_flag[j] = 1 \n",
        "                if sum(generation_end_flag) == batch_size:\n",
        "                    break\n",
        "\n",
        "        return torch.stack(predicted_sentences, dim=1)\n",
        "        \n",
        "        "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmYe4SsTetQq"
      },
      "source": [
        "# 한 -> 영\n",
        "data_ = pd.read_csv(\"/content/drive/MyDrive/data_files/korean_data/train.csv\")\n",
        "korean_data = data_[\"Korean\"].values\n",
        "english_data = data_[\"English\"].values"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRZJnw7CetQq",
        "outputId": "bd4b72c2-61f7-4e0c-ef3f-81e2fe65b943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.tokenize.word_tokenize(korean_data[100])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['그것을', '막기', '위해', ',', '많은', '과학자는', '친환경적인', '사용', '방법을', '연구했어요', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weR-02iWetQq"
      },
      "source": [
        "def build_dict(seqs):\n",
        "    num_skip_sent = 0\n",
        "    word_count = 4\n",
        "    vocab = [\"<pad>\",\"<s>\",\"</s>\",\"<unk>\"]\n",
        "    word2id = {\"<pad>\": 0, \"<s>\": 1, \"</s>\": 2, \"<unk>\": 3}\n",
        "    id2word = {0: \"<pad>\", 1: \"<s>\", 2: \"</s>\", 3: \"<unk>\"}\n",
        "    print(\"Building vocab and dict..\")\n",
        "    for line in seqs:\n",
        "        words = line.strip().split(' ') # tokenized by space \n",
        "        for word in words:\n",
        "            if word not in vocab:\n",
        "                word_count += 1 # increment word_count\n",
        "                vocab.append(word) # append new unique word\n",
        "                index = word_count - 1 # word index (consider index 0)\n",
        "                word2id[word] = index # word to index\n",
        "                id2word[index] = word # index to word\n",
        "    \n",
        "    print(\"Number of unique words: %d\" % len(vocab))\n",
        "    print(\"Finised building vocab and dict!\")\n",
        "\n",
        "    return vocab, word2id, id2word"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "et5ZhXWOYS82"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuBX1j41etQr"
      },
      "source": [
        "if os.path.isfile(\"/content/drive/MyDrive/data_files/korean_vocab.pkl\"):\n",
        "    with open(\"/content/drive/MyDrive/data_files/train_korean.pkl\", \"rb\") as f:\n",
        "        korean_data_token_ = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/data_files/train_english.pkl\", \"rb\") as f:\n",
        "        english_data_token_ = pickle.load(f)    \n",
        "    \n",
        "    with open(\"/content/drive/MyDrive/data_files/korean_vocab.pkl\", \"rb\") as f:\n",
        "        korean_vocab = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/data_files/korean_word2id.pkl\", \"rb\") as f:\n",
        "        korean_word2id = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/data_files/korean_id2word.pkl\", \"rb\") as f:\n",
        "        korean_id2word = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/data_files/english_vocab.pkl\", \"rb\") as f:\n",
        "        english_vocab = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/data_files/english_word2id.pkl\", \"rb\") as f:\n",
        "        english_word2id = pickle.load(f)\n",
        "    with open(\"/content/drive/MyDrive/data_files/english_id2word.pkl\", \"rb\") as f:\n",
        "        english_id2word = pickle.load(f)\n",
        "\n",
        "else:\n",
        "    korean_data_token = []\n",
        "    for sent in korean_data:\n",
        "        korean_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "\n",
        "    english_data_token = []\n",
        "    for sent in english_data:\n",
        "        english_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "\n",
        "    korean_data_token_ = [' '.join(token) for token in korean_data_token]\n",
        "    english_data_token_ = [' '.join(token) for token in english_data_token]    \n",
        "    \n",
        "    korean_vocab, korean_word2id, korean_id2word = build_dict(korean_data_token_)\n",
        "    english_vocab, english_word2id, english_id2word = build_dict(english_data_token_)\n",
        "    \n",
        "    pickle.dump(korean_data_token_, open(\"/content/drive/MyDrive/data_files/train_korean.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_data_token_, open(\"/content/drive/MyDrive/data_files/train_english.pkl\", \"wb\" ))    \n",
        "    \n",
        "    pickle.dump(korean_vocab, open(\"/content/drive/MyDrive/data_files/korean_vocab.pkl\", \"wb\" ))\n",
        "    pickle.dump(korean_word2id, open(\"/content/drive/MyDrive/data_files/korean_word2id.pkl\", \"wb\" ))\n",
        "    pickle.dump(korean_id2word, open(\"/content/drive/MyDrive/data_files/korean_id2word.pkl\", \"wb\" ))\n",
        "\n",
        "    pickle.dump(english_vocab, open(\"/content/drive/MyDrive/data_files/english_vocab.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_word2id, open(\"/content/drive/MyDrive/data_files/english_word2id.pkl\", \"wb\" ))\n",
        "    pickle.dump(english_id2word, open(\"/content/drive/MyDrive/data_files/english_id2word.pkl\", \"wb\" ))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efIP8QaNetQr"
      },
      "source": [
        "def batch(iterable, n=1):\n",
        "    args = [iter(iterable)] * n  # item들을 각각 묶어서 batch화 시킴 \n",
        "    return zip_longest(*args)\n",
        "\n",
        "\n",
        "def pad_tensor(vec, pad, value=0, dim=0):\n",
        "    # padding하되, 원하는 최대 길이만큼. \n",
        "    \"\"\"\n",
        "    pad token으로 채우는 용도 \n",
        "    args:\n",
        "        vec - tensor to pad\n",
        "        pad - the size to pad to\n",
        "        dim - dimension to pad\n",
        "    return:\n",
        "        a new tensor padded to 'pad' in dimension 'dim'\n",
        "    \"\"\"\n",
        "    pad_size = pad - vec.shape[0]  # 원래 갖고있는 문장길이에서, 모자란 만큼만 padding\n",
        "\n",
        "    if len(vec.shape) == 2:\n",
        "        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n",
        "    elif len(vec.shape) == 1:\n",
        "        zeros = torch.ones((pad_size,)) * value\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return torch.cat([torch.Tensor(vec), zeros], dim=dim)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBKU7nf9etQr"
      },
      "source": [
        "def pad_collate(batch, values=(0, 0), dim=0):\n",
        "    \"\"\"\n",
        "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
        "    args:\n",
        "        batch - list of (tensor, label)\n",
        "    reutrn:\n",
        "        xs - a tensor of all examples in 'batch' after padding\n",
        "        ys - a LongTensor of all labels in batch\n",
        "        ws - a tensor of sequence lengths\n",
        "    \"\"\"\n",
        "\n",
        "    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
        "    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n",
        "    target_lengths = torch.Tensor([int(x[1].shape[dim]) for x in batch])\n",
        "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
        "    src_max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
        "    tgt_max_len = max(map(lambda x: x[1].shape[dim], batch))\n",
        "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
        "    batch = [(pad_tensor(x, pad=src_max_len, dim=dim), \n",
        "              pad_tensor(y, pad=tgt_max_len, dim=dim)) for (x, y) in batch] # x는 source_max_length에 맞추어 학습. y는 target max length에 맞추어 학습\n",
        "\n",
        "    # stack all\n",
        "    xs = torch.stack([x[0] for x in batch], dim=0)\n",
        "    ys = torch.stack([x[1] for x in batch], dim=0)\n",
        "\n",
        "    xs = xs[xids].contiguous() # decreasing order로 다시 나열(내림차순) 안해도되지만, 불필요한 계산을 없애려고 해준다\n",
        "    ys = ys[xids].contiguous() # xids 와 같은 순서로 \n",
        "    target_lengths = target_lengths[xids] \n",
        "    return xs.long(), ys.long(), sequence_lengths.int(), target_lengths.int() # long type으로 입력을 받는다. \n",
        "\n",
        "\n",
        "class ToyDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    https://talbaumel.github.io/blog/attention/\n",
        "    \"\"\"\n",
        "    def __init__(self,  ko_path, en_path , ko_word2id, en_word2id):\n",
        "        with open(ko_path, \"rb\") as f:\n",
        "            self.ko_seqs = pickle.load(f)\n",
        "        with open(en_path, \"rb\") as f:\n",
        "            self.en_seqs = pickle.load(f)\n",
        "        self.ko_word2id = ko_word2id\n",
        "        self.en_word2id = en_word2id\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ko_seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ko_seqs = self.ko_seqs[index]\n",
        "        en_seqs = self.en_seqs[index]\n",
        "        ko_seqs = self.process(ko_seqs, self.ko_word2id)\n",
        "        en_seqs = self.process(en_seqs, self.en_word2id)\n",
        "        # 한국어문장(인덱스들) -> 영어문장(인덱스들)\n",
        "        return ko_seqs, en_seqs       \n",
        "\n",
        "    def process(self, seq, word2id):\n",
        "        # \n",
        "        sequence = []\n",
        "        sequence.append(word2id[\"<s>\"]) # SOS(Start of Sentence)를 넣음 \n",
        "        words = seq.strip().split(' ') # space 단위로 쪼갠다. \n",
        "        for word in words:\n",
        "            if len(sequence) < model_config.max_len:\n",
        "                if word in word2id:\n",
        "                    sequence.append(word2id[word]) # 만약 vocabulary 에 있으면, 해당 index 넣기\n",
        "                else:\n",
        "                    sequence.append(3) # replace by <unk> token\n",
        "            else:\n",
        "                break\n",
        "        sequence.append(word2id[\"</s>\"])\n",
        "        sequence = torch.Tensor(sequence)\n",
        "        return sequence"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN1gCKHpetQs"
      },
      "source": [
        "sample_config = {\n",
        "    \"dim\": 32,\n",
        "    \"dim_ff\": 32,\n",
        "    \"n_layers\": 2,\n",
        "    \"p_drop_attn\": 0.1,\n",
        "    \"n_heads\": 4,\n",
        "    \"p_drop_hidden\": 0.1,\n",
        "    \"max_len\": 30,\n",
        "    \"n_segments\": 2,\n",
        "    \"vocab_size\": 30522,\n",
        "    \"batch_size\": 32\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "\n",
        "model_config = AttributeDict(sample_config)\n",
        "model = Transformer(model_config)\n",
        "model = model.cuda()\n",
        "# out = model(sample[0].cuda(),sample[0].cuda())\n",
        "# out.size()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XST7y_CDetQs"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0) # pad 무시하는 것. pad token을 학습하면 안되잖아요."
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-Io-Gd9etQs"
      },
      "source": [
        "dataset = ToyDataset(\"/content/drive/MyDrive/data_files/train_korean.pkl\", \"/content/drive/MyDrive/data_files/train_english.pkl\", korean_word2id, english_word2id)\n",
        "train_loader = data.DataLoader(dataset, batch_size=model_config.batch_size, shuffle=True, collate_fn=pad_collate, drop_last=True)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w17hIP50etQs",
        "outputId": "85be360e-aa4e-415c-c619-50f59f3af4d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    loss_list =[]\n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        x, y, x_len, y_len = batch\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        logits = model(x, y[:,:-1]) # y에서 1개 왜 뺐을까? : </s> EOS 토큰임. 입력할 필요가 없다.\n",
        "        loss = loss_fn(logits.view(-1,len(english_vocab)) , y[:,1:].contiguous().view(-1)) # loss 구하기 우리는 cross entropy 사용.\n",
        "        # 위에서 앞에 1개 뺀 이유는 SOS 토큰을 빼주는 것 . SOS 나온 후 다음부터 label이 나오는 거니까.\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_list.append(loss.item())\n",
        "        \n",
        "        if (idx+1) % 400 == 0 :\n",
        "            print('epoch {} iteration {}/{} loss {:.4f}'.format(epoch+1, idx+1, len(train_loader), np.mean(loss_list)))\n",
        "            loss_list=[]\n",
        "            \n",
        "        if (idx+1) % 2000 == 0:\n",
        "            with torch.no_grad():\n",
        "                greedy_pred = model.greedy_decoding(x[0].unsqueeze(0)) # greedy decoding이 뭘까?=> 생성할 때 확률이 높은 순대로 만드는 것 \n",
        "            greedy_pred_indices = greedy_pred[0].data.cpu().tolist()\n",
        "            pred_indices = logits[0].argmax(-1).data.cpu().tolist()\n",
        "            input_indices = x[0].data.cpu().tolist()\n",
        "            label_indices = y[0].data.cpu().tolist()\n",
        "            \n",
        "            if 2 in pred_indices:\n",
        "                greedy_pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                greedy_pred_len = 128\n",
        "            \n",
        "            if 2 in pred_indices:\n",
        "                pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                pred_len = 128\n",
        "\n",
        "            if 2 in input_indices:\n",
        "                input_len = input_indices.index(2)\n",
        "            else:\n",
        "                input_len = 128\n",
        "\n",
        "            if 2 in label_indices:\n",
        "                label_len = label_indices.index(2)\n",
        "            else:\n",
        "                label_len = 128\n",
        "            \n",
        "            greedy_pred_words = [english_id2word[idx] for i, idx in enumerate(greedy_pred_indices) if i<= greedy_pred_len]\n",
        "            pred_words = [english_id2word[idx] for i, idx in enumerate(pred_indices) if i<=pred_len]\n",
        "            input_words = [korean_id2word[idx] for i, idx in enumerate(input_indices) if i<=input_len]\n",
        "            label_words = [english_id2word[idx] for i, idx in enumerate(label_indices) if i<=label_len]\n",
        "\n",
        "            print('=====================================')\n",
        "            print('입력:{}'.format(' '.join(input_words[1:-1])))\n",
        "            print('출력(teacher forcing):{}'.format(' '.join(pred_words[:-1])))\n",
        "            print('출력(greedy decoding):{}'.format(' '.join(greedy_pred_words[:-1])))\n",
        "            print('정답:{}'.format(' '.join(label_words[1:-1])))\n",
        "            print('=====================================')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 iteration 400/11250 loss 4.2586\n",
            "epoch 1 iteration 800/11250 loss 4.2759\n",
            "epoch 1 iteration 1200/11250 loss 4.2625\n",
            "epoch 1 iteration 1600/11250 loss 4.2547\n",
            "epoch 1 iteration 2000/11250 loss 4.2513\n",
            "=====================================\n",
            "입력:먼저 팀원들과 아이디어를 내고 가장 현명한 아이디어를 정하여 팀의 궁극적 목표를 정하였습니다 .\n",
            "출력(teacher forcing):The of the are the is the and in and in the most price of and the the opportunity . . .\n",
            "출력(greedy decoding):The most important thing that the best to be able to be a good for the best to be able to be\n",
            "정답:First , we gathered ideas with team members , settled on the wisest idea , and set an ultimate team goal .\n",
            "=====================================\n",
            "epoch 1 iteration 2400/11250 loss 4.2524\n",
            "epoch 1 iteration 2800/11250 loss 4.2291\n",
            "epoch 1 iteration 3200/11250 loss 4.2406\n",
            "epoch 1 iteration 3600/11250 loss 4.2266\n",
            "epoch 1 iteration 4000/11250 loss 4.2307\n",
            "=====================================\n",
            "입력:요즘 한국의 문화 콘텐츠 중에서 , 예능 프로그램이 중국에서 엄청난 인기를 받고 있습니다 .\n",
            "출력(teacher forcing):This these , culture , and in are , the in popular . Korea ,\n",
            "출력(greedy decoding):The Korean food is a famous culture , and the Korean food .\n",
            "정답:Among Korean cultural contents , especially entertainment TV shows are very popular in China .\n",
            "=====================================\n",
            "epoch 1 iteration 4400/11250 loss 4.2514\n",
            "epoch 1 iteration 4800/11250 loss 4.2314\n",
            "epoch 1 iteration 5200/11250 loss 4.2184\n",
            "epoch 1 iteration 5600/11250 loss 4.2367\n",
            "epoch 1 iteration 6000/11250 loss 4.2246\n",
            "=====================================\n",
            "입력:형 , 사장이 말일에 돈 준다 약속했는데 만약에 돈 안 주면 우리 다 같이 사장 찾아가서 얘기해봐야 하는 거 아냐 ?\n",
            "출력(teacher forcing):If , I first is to the the , the same time , but can we do n't work me the same to and have we the to to\n",
            "출력(greedy decoding):If you do n't have to get a little , we do n't know that we can not get to get a little ?\n",
            "정답:Hey , the boss promised to pay me on the last day , but if you do n't give me the money , we should all go talk to\n",
            "=====================================\n",
            "epoch 1 iteration 6400/11250 loss 4.2131\n",
            "epoch 1 iteration 6800/11250 loss 4.2262\n",
            "epoch 1 iteration 7200/11250 loss 4.2104\n",
            "epoch 1 iteration 7600/11250 loss 4.2087\n",
            "epoch 1 iteration 8000/11250 loss 4.1980\n",
            "=====================================\n",
            "입력:여기서는 각각의 센서별로 일정 거리 안에 들어오는 담배꽁초를 카운팅 할 수 있도록 코딩을 했습니다 .\n",
            "출력(teacher forcing):The is the can will I you can will be a in is the the year . . the . .\n",
            "출력(greedy decoding):The main of the main name of the main character , and the car can be used to the same .\n",
            "정답:Here , we coded so that cigarette butts can be counted which are within a certain distance for each sensor .\n",
            "=====================================\n",
            "epoch 1 iteration 8400/11250 loss 4.2163\n",
            "epoch 1 iteration 8800/11250 loss 4.2205\n",
            "epoch 1 iteration 9200/11250 loss 4.2134\n",
            "epoch 1 iteration 9600/11250 loss 4.1988\n",
            "epoch 1 iteration 10000/11250 loss 4.1944\n",
            "=====================================\n",
            "입력:웹 디자인은 웹과 모바일 웹 , 반응형 웹에 대한 각각의 개념을 바탕으로 PC에서 쓰이는 UI , GUI 방법론에 대해 습득합니다 .\n",
            "출력(teacher forcing):The the case , the , , the will the , the and of the the to the , and on the . to the\n",
            "출력(greedy decoding):The main character is the company of the company , and the company is the company of the company .\n",
            "정답:In the course of web design , we learn UI , GUI methodologies that are used on PC , based on concepts related to web , mobile web ,\n",
            "=====================================\n",
            "epoch 1 iteration 10400/11250 loss 4.2003\n",
            "epoch 1 iteration 10800/11250 loss 4.2166\n",
            "epoch 1 iteration 11200/11250 loss 4.2007\n",
            "epoch 2 iteration 400/11250 loss 4.1059\n",
            "epoch 2 iteration 800/11250 loss 4.0928\n",
            "epoch 2 iteration 1200/11250 loss 4.1079\n",
            "epoch 2 iteration 1600/11250 loss 4.1237\n",
            "epoch 2 iteration 2000/11250 loss 4.0899\n",
            "=====================================\n",
            "입력:나는 한창 진로에 고민이 많을 고1이라는 나이에 도움을 줄 수 있는 프로그램들이 더 많아지기를 기대해요 .\n",
            "출력(teacher forcing):I can to than to I to school , , to can a and the the lot .\n",
            "출력(greedy decoding):I think the reason that I can have a lot of the first time to be able to\n",
            "정답:I expect more programs that help high school first graders who have concerns about choosing a career .\n",
            "=====================================\n",
            "epoch 2 iteration 2400/11250 loss 4.1046\n",
            "epoch 2 iteration 2800/11250 loss 4.1093\n",
            "epoch 2 iteration 3200/11250 loss 4.1069\n",
            "epoch 2 iteration 3600/11250 loss 4.1023\n",
            "epoch 2 iteration 4000/11250 loss 4.0950\n",
            "=====================================\n",
            "입력:무엇보다 혼자서 책으로 영어 공부를 하는 것보다 외국인들과 직접 부딪혀 실무능력을 키우는 것이 큰 메리트라고 생각합니다 .\n",
            "출력(teacher forcing):I reason to that a do a in , the , I than the . my English .\n",
            "출력(greedy decoding):I think I was very important to learn about the English and English is very difficult .\n",
            "정답:The best thing is to not study English by books but rather through communication with other people .\n",
            "=====================================\n",
            "epoch 2 iteration 4400/11250 loss 4.1103\n",
            "epoch 2 iteration 4800/11250 loss 4.0677\n",
            "epoch 2 iteration 5200/11250 loss 4.1056\n",
            "epoch 2 iteration 5600/11250 loss 4.0993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ExFwHofUetQs"
      },
      "source": [
        "#결과값 출력\n",
        "model.eval()\n",
        "for idx, batch in enumerate(train_loader):\n",
        "    x, y, x_len, y_len = batch\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "    pred = model.greedy_decoding(x)\n",
        "\n",
        "    for i in range(pred.shape[0]):\n",
        "        pred_indices = pred[i].data.cpu().tolist()\n",
        "        input_indices = x[i].data.cpu().tolist()\n",
        "        label_indices = y[i].data.cpu().tolist()\n",
        "\n",
        "        if 2 in pred_indices:\n",
        "            pred_len = pred_indices.index(2)\n",
        "        else:\n",
        "            pred_len = 128\n",
        "\n",
        "        if 2 in input_indices:\n",
        "            input_len = input_indices.index(2)\n",
        "        else:\n",
        "            input_len = 128\n",
        "        \n",
        "        if 2 in label_indices:\n",
        "            label_len = label_indices.index(2)\n",
        "        else:\n",
        "            label_len = 128\n",
        "        \n",
        "        pred_words = [english_id2word[idx] for i,idx in enumerate(pred_indices) if i <= pred_len]\n",
        "        input_words = [korean_id2word[idx] for i,idx in enumerate(input_indices) if i <= input_len]\n",
        "        label_words = [english_id2word[idx] for i,idx in enumerate(label_indices) if i <= label_len]\n",
        "        if pred_len>1:\n",
        "            print('===================')\n",
        "            print('입력:{}'.format(' '.join(input_words)))\n",
        "            print('출력(greedy decoding):{}'.format(' '.join(pred_words)))\n",
        "            print('답안:{}'.format(' '.join(label_words)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lnti33cetQt"
      },
      "source": [
        "# model save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2E8EQ4jetQt"
      },
      "source": [
        "path = 'trained_model.pth'\n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w97VaEnaetQt"
      },
      "source": [
        "## model load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jol3wa3jetQt"
      },
      "source": [
        "path = 'trained_model.pth'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTkUDvtietQt"
      },
      "source": [
        "## TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbiBFitsetQu"
      },
      "source": [
        "def test_pad_collate(batch, values=(0, 0), dim=0):\n",
        "    \"\"\"\n",
        "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
        "    args:\n",
        "        batch - list of (tensor, label)\n",
        "    reutrn:\n",
        "        xs - a tensor of all examples in 'batch' after padding\n",
        "        ys - a LongTensor of all labels in batch\n",
        "        ws - a tensor of sequence lengths\n",
        "    \"\"\"\n",
        "\n",
        "    sequence_lengths = torch.Tensor([int(x.shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
        "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
        "    src_max_len = max(map(lambda x: x.shape[dim], batch))\n",
        "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
        "    batch = [pad_tensor(x, pad=src_max_len, dim=dim) for x in batch]\n",
        "\n",
        "    # stack all\n",
        "    xs = torch.stack(batch, dim=0)\n",
        "    return xs.long(), sequence_lengths.int()\n",
        "\n",
        "class TestDataset(data.Dataset):\n",
        "    def __init__(self,  ko_path, ko_word2id):\n",
        "        self.load_data(ko_path)\n",
        "        self.ko_word2id = ko_word2id\n",
        "        \n",
        "        \n",
        "    def load_data(self, ko_path):\n",
        "        korean_test_data_ = pd.read_csv(\"./korean_data/test_korean.csv\")\n",
        "        korean_test_data =korean_test_data_['Korean'].iloc[:2000].values\n",
        "        korean_test_data_token = []\n",
        "        for sent in korean_test_data:\n",
        "            korean_test_data_token.append([token for token in nltk.tokenize.word_tokenize(sent)])\n",
        "        korean_test_data_token = [' '.join(token) for token in korean_test_data_token]\n",
        "        self.ko_seqs = korean_test_data_token\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ko_seqs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ko_seqs = self.ko_seqs[index]\n",
        "        ko_seqs = self.process(ko_seqs, self.ko_word2id)\n",
        "        return ko_seqs       \n",
        "\n",
        "    def process(self, seq, word2id):\n",
        "        sequence = []\n",
        "        sequence.append(word2id[\"<s>\"])\n",
        "        words = seq.strip().split(' ')\n",
        "        for word in words:\n",
        "            if word in word2id:\n",
        "                sequence.append(word2id[word]) # \n",
        "            else:\n",
        "                sequence.append(3) # replace by <unk> token\n",
        "        sequence.append(word2id[\"</s>\"])\n",
        "        sequence = torch.Tensor(sequence)\n",
        "        return sequence\n",
        "    \n",
        "    \n",
        "def test(model):\n",
        "    \n",
        "    with open(\"./korean_word2id.pkl\", \"rb\") as f:\n",
        "        korean_word2id = pickle.load(f)\n",
        "    \n",
        "    test_dataset = TestDataset('./test_korean.pkl', korean_word2id)\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=model_config.batch_size, shuffle=False, collate_fn=test_pad_collate, drop_last=False)\n",
        "    model.eval()\n",
        "    j = 0\n",
        "    \n",
        "    f = open('prediction_result.txt', 'w')\n",
        "    for idx, batch in enumerate(test_loader):\n",
        "        x, x_len= batch\n",
        "        x = x.cuda()\n",
        "        pred = model.greedy_decoding(x)\n",
        "\n",
        "        for i in range(pred.shape[0]):\n",
        "            j+=1\n",
        "            \n",
        "            pred_indices = pred[i].data.cpu().tolist()\n",
        "            input_indices = x[i].data.cpu().tolist()\n",
        "\n",
        "            if 2 in pred_indices:\n",
        "                pred_len = pred_indices.index(2)\n",
        "            else:\n",
        "                pred_len = 128\n",
        "\n",
        "            if 2 in input_indices:\n",
        "                input_len = input_indices.index(2)\n",
        "            else:\n",
        "                input_len = 128\n",
        "\n",
        "            pred_words = [english_id2word[idx] for i,idx in enumerate(pred_indices) if i <= pred_len]\n",
        "            input_words = [korean_id2word[idx] for i,idx in enumerate(input_indices) if i <= input_len]\n",
        "            \n",
        "            if j%50 == 0 :\n",
        "                print('========== index {} ========='.format(j))\n",
        "                print('입력:{}'.format(' '.join(input_words[1:-1])))\n",
        "                print('출력:{}'.format(' '.join(pred_words[:-1])))\n",
        "\n",
        "            f.write(' '.join(pred_words[:-1]))\n",
        "            f.write('\\n')\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Em4iwSp8etQu"
      },
      "source": [
        "test(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EjRI_zQetQu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8DHAo9QetQu"
      },
      "source": [
        "# 과제\n",
        "\n",
        "1. query, key, value 간의 관계에 대해 작성해주세요.\n",
        "- query는 '요구사항'으로, 조건이라고 할 수 있으며 그 값은 key 중 하나이다. key는 토큰에 대한 index와 비슷하다. value는 key와 연결된 실제 토큰이자, 값이다\n",
        "2. multi-head 의 필요성에 대해 작성해주세요.\n",
        "- 여러개의 attetion을 만들어 입력받은 query, key, value를 병렬적으로 계산해준다. 병렬적으로 수행하면 쿼리 여러개에서 연관도를 더 신뢰성있게 구할 수 있다. \n",
        "3. BERT 사전 학습의 두가지 태스크에 대해 설명해주세요.\n",
        "- MLM : 입력 문장에서 임의로 dropout하고[mask], 그 masking된 토큰을 맞춘다.\n",
        "- NSP : A, B 라는 문장이 주어지면 무엇이 더 먼저 나와야하는지 순서를 맞추는 방식.\n",
        "4. transformer embedding과 BERT embedding의 차이점 두가지를 작성해 주세요\n",
        "- BERT는 Transformer의 인코더만을 이용한다. 디코더가 존재하지 않는다.\n",
        "- BERT는 토큰의 순서에 상관 없이 예측을 한다."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}