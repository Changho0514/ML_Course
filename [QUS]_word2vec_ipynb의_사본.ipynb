{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[QUS] word2vec.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Changho0514/web1/blob/main/%5BQUS%5D_word2vec_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FAK0fz1kOr"
      },
      "source": [
        "## **Word2Vec (CBOW, Skip-gram)**\n",
        "1. 라이브러리를 활용하지 않고 CBOW, Skip-gram 모델을 각각 구현, 학습합니다.\n",
        "3. gensim 라이브러리를 활용하여 CBOW, Skip-gram 모델을 학습하고 시각화합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-GScBzqi9nl"
      },
      "source": [
        "## **1. Word2Vec 구현 및 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FrxTPWIsct"
      },
      "source": [
        "### 1.1 필요한 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjroCdtwI9Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb29e2c-5f45-412c-9f30-77d59e037bbe"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP7aXfJIr3i"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from konlpy.tag import Okt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugro74yJASr"
      },
      "source": [
        "### 1.2 Dataset 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36dfSRRJDtX"
      },
      "source": [
        "Sample 데이터를 확인합니다.\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ2f-lRJSus"
      },
      "source": [
        "train_data = [\n",
        "  \"정말 맛있습니다. 추천합니다.\",\n",
        "  \"기대했던 것보단 별로였네요.\",\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \n",
        "]\n",
        "\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReElaFSLBYL"
      },
      "source": [
        "KoNLPy 패키지에서 제공하는 Twitter(Okt) tokenizer를 사용하여 tokenization합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjlRzmWMDK_"
      },
      "source": [
        "tokenizer = Okt()   # 단어 분리하기 위한 툴. 기본형으로 저장됨 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTUsX672icp"
      },
      "source": [
        "def make_tokenized(data):\n",
        "    tokenized = []\n",
        "    for sent in tqdm(data):\n",
        "        tokens = tokenizer.morphs(sent, stem=True)\n",
        "        tokenized.append(tokens)\n",
        "\n",
        "    return tokenized"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-z0z6HD2rrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d916be9f-6337-4cbe-97ef-e01ce59a462a"
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:11<00:00,  1.15s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenize된 데이터를 확인해봅시다. "
      ],
      "metadata": {
        "id": "ThRsnf47J011"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfv5jGrqk-Nb",
        "outputId": "39a293e4-cf3c-4cfb-f6a4-1a78d164897d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_tokenized"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['정말', '맛있다', '.', '추천', '하다', '.'],\n",
              " ['기대하다', '것', '보단', '별로', '이다', '.'],\n",
              " ['다',\n",
              "  '좋다',\n",
              "  '가격',\n",
              "  '이',\n",
              "  '너무',\n",
              "  '비싸다',\n",
              "  '다시',\n",
              "  '가다',\n",
              "  '싶다',\n",
              "  '생각',\n",
              "  '이',\n",
              "  '안',\n",
              "  '드네',\n",
              "  '요',\n",
              "  '.'],\n",
              " ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'],\n",
              " ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'],\n",
              " ['위생',\n",
              "  '상태',\n",
              "  '가',\n",
              "  '좀',\n",
              "  '별로',\n",
              "  '이다',\n",
              "  '.',\n",
              "  '좀',\n",
              "  '더',\n",
              "  '개선',\n",
              "  '되다',\n",
              "  '기르다',\n",
              "  '바라다',\n",
              "  '.'],\n",
              " ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'],\n",
              " ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'],\n",
              " ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'],\n",
              " ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzQSjEr1lIhh"
      },
      "source": [
        "Tokenize 된 단어의 빈도 수를 확인할 수 있는 `word_count` 를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51exEpI0Mc3l",
        "outputId": "d3ff48f5-7c91-4815-aa93-4d7c2e1d6802",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count = defaultdict(int) #default값이 int인 딕셔너리, 값을 지정하지 않았다면 0으로 지정됨\n",
        "\n",
        "for tokens in tqdm(train_tokenized):\n",
        "    for token in tokens:\n",
        "        word_count[token] += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 828.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBnhK-WVlXvM"
      },
      "source": [
        "가장 많이 등장한 토큰부터 적게 등장한 토큰 순으로 나열하여 `word_count` 를 확인해 봅니다.  \n",
        "목적: 각 토큰에 대해서 빈도수를 보고싶은 것.  \n",
        "딕셔너리로 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvHAMAnMh1D",
        "outputId": "8e9e1fa4-1cfc-41ed-f644-99cdd758f986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "print(list(word_count))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czbecydTlhSO"
      },
      "source": [
        "각 단어를 정수 값(index) 에 mapping 하는 word to index (`w2i`) 를 생성합니다.\n",
        "\n",
        "`w2i` 의 전체 크기는 고유한 단어의 개수(vocab size)와 동일합니다.\n",
        "\n",
        "- 가장 많이 나온 단어가 정수 0, 적게 나온 단어가 정수 n을 부여받도록 해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaK_i3zL2vO3",
        "outputId": "964d49c0-e59f-4495-873d-ec7de4381e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w2i = {}\n",
        "for pair in tqdm(word_count):\n",
        "    ## TO DO (start) ##\n",
        "    key, value = pair\n",
        "    w2i[key] = len(w2i)\n",
        "    ## TO DO (end) ##"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 219023.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiGqiEGDL5B_",
        "outputId": "3fd50613-12b8-40ba-ce02-4c3244fcd5b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(train_tokenized)\n",
        "print(w2i)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
            "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDb2LUmF3sZE"
      },
      "source": [
        "실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIgzTw_OvBc6"
      },
      "source": [
        "**Tip** `torch.utils.data.Dataset`\n",
        "\n",
        "\n",
        "---\n",
        "파이토치에서는 데이터를 좀 더 쉽게 다룰 수 있도록 Dataset과 DataLoader를 제공합니다.\n",
        "\n",
        "이 도구를 활용하면 미니 배치 학습, 데이터 셔플, 병렬 처리를 간단히 수행할 수 있습니다. \n",
        "\n",
        "Dataset 를 정의하고 이를 DataLoader 에 전달함으로써 사용할 수 있습니다.\n",
        "\n",
        "파이토치에서 제공하는 Dataset 을 커스텀하여 `CBOWDataset` 과 `SkipGramDataset` 을 정의해 봅시다.\n",
        "\n",
        "`__init__`, `__len__`, `__getitem__` 을 다시 정의함으로써 커스텀할 수 있습니다.\n",
        "\n",
        "출처: https://wikidocs.net/55580\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXA5zaPPM3Wd"
      },
      "source": [
        "![w2vec_1.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW는 주변 단어를 input으로 넣어서 output으로 중심 단어를 유추한다  \n",
        "skip-gram은 중심 단어를 Input으로 넣어서 output으로 주변 단어를 유추한다"
      ],
      "metadata": {
        "id": "d0l52Vqd3Y5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 나는 너를 사랑한다  \n",
        "에서 '너를'이 중심단어, 주변단어는 '나는', '사랑한다'"
      ],
      "metadata": {
        "id": "_klkgc_vxtyM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6llG2ZYPmo7Y"
      },
      "source": [
        "CBOW 는 window 내에 있는 context word를 가지고 target word 하나를 맞추는 과정에서 학습됩니다.\n",
        "\n",
        "Skip-gram 은 target word 하나를 가지고 주변 context word 를 맞추는 과정에서 학습됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47ssyVt89t1"
      },
      "source": [
        "# CBOW는 중심단어를 주고(self.y), 주변 단어는 self.x에 들어간다\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, train_tokenized, window_size=2): #window_size는 \"정말 맛있습니다 매우 추천합니다 굿\" 중심단어가 '매우' 인데 이 앞뒤로 2개 사이즈를 주변단어로 생각한다는 것 \n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        \n",
        "        ## TO DO (start) ##\n",
        "        for tokens in train_tokenized:\n",
        "          # 1. token => integer (w2i 딕셔너리 활용)\n",
        "          token_ids = [w2i[token] for token in tokens]\n",
        "          # print(token_ids)\n",
        "\n",
        "          # 2. 중심단어, 주변단어 -> 리스트에 넣기\n",
        "          for i, id in enumerate(token_ids):\n",
        "            # 2. 1 중심단어가 될 수 있는지?        \n",
        "            if i >= window_size and i < (len(token_ids)-window_size):\n",
        "              # 나의 답\n",
        "              # self.x.append(token_ids[i+1][1], token_ids[i+2][1], token_ids[i-1][1], token_ids[i-2][1])\n",
        "              self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\n",
        "              self.y.append(id)\n",
        "        ## TO DO (end) ##\n",
        "        \n",
        "        self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\n",
        "        self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[17, 18, 0, 19, 10, 0]  \n",
        "[20, 21, 22, 4, 2, 0]  \n",
        "[5, 3, 23, 6, 7, 24, 25, 26, 27, 28, 6, 29, 30, 31, 0]  \n",
        "[32, 33, 2, 34, 35, 11, 36, 37, 0]  \n",
        "[8, 1, 9, 1, 5, 38, 0]  \n",
        "[12, 39, 40, 13, 4, 2, 0, 13, 14, 41, 42, 43, 44, 0]  \n",
        "[45, 1, 3, 46, 47, 9, 1, 7, 48, 0]  \n",
        "[49, 15, 11, 10, 8, 1, 50, 1, 9, 1, 5, 3, 0]  \n",
        "[51, 52, 53, 8, 6, 7, 54, 0, 55, 56, 4, 2, 0]  \n",
        "[12, 15, 16, 14, 57, 58, 3, 0, 16, 59, 0]  \n",
        "[]\n",
        "첫줄 에서 17, 18, 10, 0 은 중심단어가 될 수 없다"
      ],
      "metadata": {
        "id": "jxzkr_zZ0VpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\n",
        "print(list(cbow_set))"
      ],
      "metadata": {
        "id": "QDbAF-6jmAV3",
        "outputId": "98bef823-f316-485a-ea19-2684ccbf1053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor([17, 18, 19, 10]), tensor(0)), (tensor([18,  0, 10,  0]), tensor(19)), (tensor([20, 21,  4,  2]), tensor(22)), (tensor([21, 22,  2,  0]), tensor(4)), (tensor([5, 3, 6, 7]), tensor(23)), (tensor([ 3, 23,  7, 24]), tensor(6)), (tensor([23,  6, 24, 25]), tensor(7)), (tensor([ 6,  7, 25, 26]), tensor(24)), (tensor([ 7, 24, 26, 27]), tensor(25)), (tensor([24, 25, 27, 28]), tensor(26)), (tensor([25, 26, 28,  6]), tensor(27)), (tensor([26, 27,  6, 29]), tensor(28)), (tensor([27, 28, 29, 30]), tensor(6)), (tensor([28,  6, 30, 31]), tensor(29)), (tensor([ 6, 29, 31,  0]), tensor(30)), (tensor([32, 33, 34, 35]), tensor(2)), (tensor([33,  2, 35, 11]), tensor(34)), (tensor([ 2, 34, 11, 36]), tensor(35)), (tensor([34, 35, 36, 37]), tensor(11)), (tensor([35, 11, 37,  0]), tensor(36)), (tensor([8, 1, 1, 5]), tensor(9)), (tensor([ 1,  9,  5, 38]), tensor(1)), (tensor([ 9,  1, 38,  0]), tensor(5)), (tensor([12, 39, 13,  4]), tensor(40)), (tensor([39, 40,  4,  2]), tensor(13)), (tensor([40, 13,  2,  0]), tensor(4)), (tensor([13,  4,  0, 13]), tensor(2)), (tensor([ 4,  2, 13, 14]), tensor(0)), (tensor([ 2,  0, 14, 41]), tensor(13)), (tensor([ 0, 13, 41, 42]), tensor(14)), (tensor([13, 14, 42, 43]), tensor(41)), (tensor([14, 41, 43, 44]), tensor(42)), (tensor([41, 42, 44,  0]), tensor(43)), (tensor([45,  1, 46, 47]), tensor(3)), (tensor([ 1,  3, 47,  9]), tensor(46)), (tensor([ 3, 46,  9,  1]), tensor(47)), (tensor([46, 47,  1,  7]), tensor(9)), (tensor([47,  9,  7, 48]), tensor(1)), (tensor([ 9,  1, 48,  0]), tensor(7)), (tensor([49, 15, 10,  8]), tensor(11)), (tensor([15, 11,  8,  1]), tensor(10)), (tensor([11, 10,  1, 50]), tensor(8)), (tensor([10,  8, 50,  1]), tensor(1)), (tensor([8, 1, 1, 9]), tensor(50)), (tensor([ 1, 50,  9,  1]), tensor(1)), (tensor([50,  1,  1,  5]), tensor(9)), (tensor([1, 9, 5, 3]), tensor(1)), (tensor([9, 1, 3, 0]), tensor(5)), (tensor([51, 52,  8,  6]), tensor(53)), (tensor([52, 53,  6,  7]), tensor(8)), (tensor([53,  8,  7, 54]), tensor(6)), (tensor([ 8,  6, 54,  0]), tensor(7)), (tensor([ 6,  7,  0, 55]), tensor(54)), (tensor([ 7, 54, 55, 56]), tensor(0)), (tensor([54,  0, 56,  4]), tensor(55)), (tensor([ 0, 55,  4,  2]), tensor(56)), (tensor([55, 56,  2,  0]), tensor(4)), (tensor([12, 15, 14, 57]), tensor(16)), (tensor([15, 16, 57, 58]), tensor(14)), (tensor([16, 14, 58,  3]), tensor(57)), (tensor([14, 57,  3,  0]), tensor(58)), (tensor([57, 58,  0, 16]), tensor(3)), (tensor([58,  3, 16, 59]), tensor(0)), (tensor([ 3,  0, 59,  0]), tensor(16))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvInhQ33AMJv"
      },
      "source": [
        "class SkipGramDataset(Dataset):\n",
        "    def __init__(self, train_tokenized, window_size=2):\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "\n",
        "        ## TO DO ##\n",
        "        # start #\n",
        "        for tokens in train_tokenized:\n",
        "          # 1. token => integer (w2i 딕셔너리 활용)\n",
        "          token_ids = [w2i[token] for token in tokens]\n",
        "          # print(token_ids)\n",
        "\n",
        "          # 2. 중심단어, 주변단어 -> 리스트에 넣기\n",
        "          for i, id in enumerate(token_ids):\n",
        "            # 2.1 중심단어가 될 수 있는지?        \n",
        "            if i >= window_size and i < (len(token_ids)-window_size):\n",
        "              \n",
        "              #2.2 여기!\n",
        "              # (x중심, y주변)으로 짜기\n",
        "              for k in range(1, window_size+1):\n",
        "                self.x.append(id) \n",
        "                self.y.append(token_ids[i-k])\n",
        "                self.x.append(id)\n",
        "                self.y.append(token_ids[i+k])\n",
        "              # self.x =    # 중심 단어\n",
        "              # self.y      # 주변 단어\n",
        "        #  end  #\n",
        "        self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\n",
        "        self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ep7Hm6oBWyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "967c1c7f-6b4d-4ee0-a326-d01c650d6d8f"
      },
      "source": [
        "skipgram_set = SkipGramDataset(train_tokenized)\n",
        "print(list(skipgram_set))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(17)), (tensor(0), tensor(10)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(20)), (tensor(22), tensor(2)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(0)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(5)), (tensor(23), tensor(7)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(24)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(25)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(26)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(27)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(28)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(6)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(29)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(30)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(31)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(0)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(32)), (tensor(2), tensor(35)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(11)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(36)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(37)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(0)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(8)), (tensor(9), tensor(5)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(38)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(0)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(12)), (tensor(40), tensor(4)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(2)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(0)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(13)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(14)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(41)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(42)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(43)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(44)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(0)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(45)), (tensor(3), tensor(47)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(9)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(1)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(7)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(48)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(0)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(49)), (tensor(11), tensor(8)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(1)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(50)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(9)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(5)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(3)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(0)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(51)), (tensor(53), tensor(6)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(7)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(54)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(0)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(55)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(56)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(4)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(2)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(0)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(12)), (tensor(16), tensor(57)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(58)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(3)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(0)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(16)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(59)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSo73PoRyd9"
      },
      "source": [
        "### 1.3 모델 Class 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnk44R6R28x"
      },
      "source": [
        "차례대로 두 가지 Word2Vec 모델을 구현합니다.  \n",
        "\n",
        "\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NryIX9-LpCwv"
      },
      "source": [
        "**Tip** `nn.Embedding` \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "위에서 단어(토큰)을 고유한 정수 값으로 매핑하기 위해 정의한 `w2i` 를 떠올려 봅시다. <br>\n",
        "입력 시퀀스의 단어를 모델의 입력으로 사용하기 위해서는,\n",
        "\n",
        "1. `w2i` 를 이용해 각 단어를 고유한 정수 값으로 인코딩합니다.\n",
        "2. 인코딩된 정수 시퀀스를 임베딩 층에 통과시킵니다.\n",
        "3. 입력 시퀀스의 길이와 동일한 개수의 임베딩 벡터를 얻습니다. <br>\n",
        "\n",
        "`nn.Embedding` 은 전체 어휘 집합 개수의 임베딩 벡터들 가운데 각 단어의 인덱스에 해당한는 임베딩 벡터에 접근하는 lookup table 로써 2번을 할 수 있도록 해줍니다. \n",
        "\n",
        "임베딩 벡터는 훈련 과정에서 학습됩니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkZhzw0NrXvQ"
      },
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "출처: https://wikidocs.net/64779"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtOvDGWGtG-w"
      },
      "source": [
        "`nn.Embedding` 의 input 과 output 은 다음과 같습니다.\n",
        "\n",
        "\n",
        "*   Input: (*), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
        "*   Output: (*, $H$), where * is the input shape and $H =\\text{embediding_dim}$\n",
        "\n",
        "```\n",
        "# an Embedding module containing 10 tensors of size 3\n",
        "embedding = nn.Embedding(10, 3)\n",
        "# a batch of 2 samples of 4 indices each\n",
        "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
        "embedding(input)\n",
        "tensor([[[-0.0251, -1.6902,  0.7172],\n",
        "         [-0.6431,  0.0748,  0.6969],\n",
        "         [ 1.4970,  1.3448, -0.9685],\n",
        "         [-0.3677, -2.7265, -0.1685]],\n",
        "\n",
        "        [[ 1.4970,  1.3448, -0.9685],\n",
        "         [ 0.4362, -0.4004,  0.9400],\n",
        "         [-0.6431,  0.0748,  0.6969],\n",
        "         [ 0.9124, -2.3616,  1.1151]]])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "출처: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(10, 3) # '10개의 3차원 벡터를 만들어줘'\n",
        "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
        "embedding(input)"
      ],
      "metadata": {
        "id": "NhfB2UFPLhFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4748eab2-79f4-44a1-e65b-47388fb4f0ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.7407,  1.6363, -0.4832],\n",
              "         [ 1.0367, -0.0155,  0.9151],\n",
              "         [ 0.7659, -0.3500,  0.0325],\n",
              "         [ 1.4713,  0.2392, -0.8692]],\n",
              "\n",
              "        [[ 0.7659, -0.3500,  0.0325],\n",
              "         [-0.0398,  0.4841,  0.6937],\n",
              "         [ 1.0367, -0.0155,  0.9151],\n",
              "         [ 0.4129,  0.3048,  0.9201]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HP1ISq5CWv"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "        self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "    # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "    def forward(self, x):  # x: (B, 2W)\n",
        "        \n",
        "        # import pdb; pdb.set_trace()\n",
        "        ## TO DO ##\n",
        "        # start #\n",
        "\n",
        "        # 1. embedding => 벡터 표현 얻기\n",
        "        embeddings = self.embedding(x) #벡터 표현을 갖고있을 변수 embeddings 설정\n",
        "        # 2. 벡터들을 더해주기\n",
        "        embeddings = torch.sum(embeddings, dim=1) # [64, 4, 256]에서 4가 의미하는게 주변 단어 4개(windows=2)였으니까\n",
        "        # 3. linear layer //embedding = [64, 256]이 됨 \n",
        "        output = self.linear(embeddings) #[64, 60]이 된다. 60개의 단어를 갖는다는 뜻  \n",
        "\n",
        "        #  end  #\n",
        "\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_loader = DataLoader(cbow_set, batch_size=64)\n",
        "sample_x, sample_y = next(iter(cbow_loader))\n",
        "cbow = CBOW(vocab_size=len(w2i), dim=256)\n",
        "cbow(sample_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9P-C5X4CmSK",
        "outputId": "a69cff8b-440a-4973-e52f-2305e7bba8f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.6110,  1.9637,  0.6831,  ...,  2.0081,  0.5069, -0.4347],\n",
              "        [ 0.9656,  2.6975, -2.0452,  ...,  3.2632,  1.0581, -1.1832],\n",
              "        [ 0.1477,  1.3616,  0.3752,  ..., -0.5112, -1.6775,  0.1857],\n",
              "        ...,\n",
              "        [-0.2580, -1.1834, -1.2366,  ...,  1.4980,  0.0056, -0.9903],\n",
              "        [-1.3180, -2.1481, -1.1824,  ...,  0.6805,  1.1052, -0.5184],\n",
              "        [-0.1753, -0.0069, -2.7139,  ...,  2.6813,  1.8333, -0.5379]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQAUApww68MJ"
      },
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\n",
        "        self.linear = nn.Linear(dim, vocab_size)\n",
        "\n",
        "    # B: batch size, W: window size, d_w: word embedding size, V: vocab size\n",
        "    def forward(self, x): # x: (B)\n",
        "\n",
        "        ## TO DO ##\n",
        "        # start #\n",
        "        # import pdb; pdb.set_trace()\n",
        "        embeddings = self.embedding(x)\n",
        "        output = self.linear(embeddings)\n",
        "\n",
        "        #  end  #\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_loader = DataLoader(skipgram_set, batch_size=64)\n",
        "sample_x, sample_y = next(iter(skipgram_loader))\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)\n",
        "skipgram(sample_x)"
      ],
      "metadata": {
        "id": "IsYZnYZYQJ5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da46d044-7114-41ff-e272-68f6d264b6cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4324,  0.9707,  0.6968,  ...,  0.5606,  0.3212,  0.4937],\n",
              "        [ 0.4324,  0.9707,  0.6968,  ...,  0.5606,  0.3212,  0.4937],\n",
              "        [ 0.4324,  0.9707,  0.6968,  ...,  0.5606,  0.3212,  0.4937],\n",
              "        ...,\n",
              "        [-0.5278,  0.0083, -0.6244,  ...,  0.4368,  0.5991, -0.2906],\n",
              "        [-0.5278,  0.0083, -0.6244,  ...,  0.4368,  0.5991, -0.2906],\n",
              "        [-0.5278,  0.0083, -0.6244,  ...,  0.4368,  0.5991, -0.2906]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP7qdtNWil1"
      },
      "source": [
        "### 1.4 모델 학습 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVggZrQ4WpBS"
      },
      "source": [
        "다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtpkIeCnuwd5"
      },
      "source": [
        "**Tip** `torch.utils.data.DataLoader` \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Dataloader 는 앞서 정의한 Dataset 를 전달 받아 학습 과정에서 모델의 입력으로 사용할 수 있도록 데이터를 load 해줍니다. \n",
        "\n",
        "batch_size 를 정해줌으로써 미니 배치 학습이 가능한 형태로 데이터를 가공합니다.\n",
        "\n",
        "예를 들어, 전체 데이터 수가 100 개 이고 batch size 가 20인 경우, 데이터는 20개의 데이터가 하나의 배치로 묶여 총 5개 의 배치로 구성됩니다.\n",
        "즉, (100, ) 사이즈의 데이터가 (5, 20, ) 로 변하게 됩니다.\n",
        "\n",
        "모델을 학습할 때에는 for loop 을 돌면서 한 iteration 에 하나의 배치를 데이터 로더로부터 전달 받아 모델의 입력으로 사용합니다.\n",
        "\n",
        "\n",
        "```\n",
        "# Example of DataLoader usage\n",
        "for batch in data_loader:\n",
        "    output = model(batch)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVdz5rSBeNu"
      },
      "source": [
        "batch_size=4\n",
        "learning_rate = 5e-4\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekixqKB3X5C1"
      },
      "source": [
        "첫번째로 CBOW 모델 학습입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d95qR7oC822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a58b75f-3dc2-4a8e-b52d-89002f881a82"
      },
      "source": [
        "cbow.train()\n",
        "cbow = cbow.to(device)\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "    print(\"#\" * 50)\n",
        "    print(f\"Epoch: {e}\")\n",
        "    for batch in tqdm(cbow_loader):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "        output = cbow(x)  # (B, V)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss = loss_function(output, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 171.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.3851635456085205\n",
            "Train loss: 4.056219100952148\n",
            "Train loss: 3.857046127319336\n",
            "Train loss: 5.5737199783325195\n",
            "Train loss: 3.678243637084961\n",
            "Train loss: 4.918858528137207\n",
            "Train loss: 5.32630729675293\n",
            "Train loss: 4.224360466003418\n",
            "Train loss: 4.713920593261719\n",
            "Train loss: 5.161181926727295\n",
            "Train loss: 4.6192240715026855\n",
            "Train loss: 5.4019670486450195\n",
            "Train loss: 5.395505428314209\n",
            "Train loss: 4.666454315185547\n",
            "Train loss: 4.6583051681518555\n",
            "Train loss: 5.14893913269043\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 339.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.2208900451660156\n",
            "Train loss: 3.927328586578369\n",
            "Train loss: 3.7246780395507812\n",
            "Train loss: 5.440969467163086\n",
            "Train loss: 3.567523956298828\n",
            "Train loss: 4.629146575927734\n",
            "Train loss: 5.119905471801758\n",
            "Train loss: 4.081268310546875\n",
            "Train loss: 4.579463005065918\n",
            "Train loss: 4.9616193771362305\n",
            "Train loss: 4.419289588928223\n",
            "Train loss: 5.012302398681641\n",
            "Train loss: 5.241763114929199\n",
            "Train loss: 4.552600860595703\n",
            "Train loss: 4.482043266296387\n",
            "Train loss: 5.018763065338135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.0613045692443848\n",
            "Train loss: 3.8020520210266113\n",
            "Train loss: 3.5947835445404053\n",
            "Train loss: 5.3092875480651855\n",
            "Train loss: 3.4586520195007324\n",
            "Train loss: 4.347187042236328\n",
            "Train loss: 4.918243408203125\n",
            "Train loss: 3.940906286239624\n",
            "Train loss: 4.448957920074463\n",
            "Train loss: 4.7664408683776855\n",
            "Train loss: 4.227231979370117\n",
            "Train loss: 4.635112762451172\n",
            "Train loss: 5.09055233001709\n",
            "Train loss: 4.441958427429199\n",
            "Train loss: 4.309638023376465\n",
            "Train loss: 4.891375541687012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 16/16 [00:00<00:00, 446.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 234.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.9066338539123535\n",
            "Train loss: 3.6802449226379395\n",
            "Train loss: 3.4674081802368164\n",
            "Train loss: 5.178668975830078\n",
            "Train loss: 3.351644515991211\n",
            "Train loss: 4.073721885681152\n",
            "Train loss: 4.721580505371094\n",
            "Train loss: 3.803359031677246\n",
            "Train loss: 4.322239875793457\n",
            "Train loss: 4.575927734375\n",
            "Train loss: 4.04336404800415\n",
            "Train loss: 4.272713661193848\n",
            "Train loss: 4.941777229309082\n",
            "Train loss: 4.334421157836914\n",
            "Train loss: 4.141435623168945\n",
            "Train loss: 4.766700267791748\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 385.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.7571141719818115\n",
            "Train loss: 3.5617713928222656\n",
            "Train loss: 3.342599391937256\n",
            "Train loss: 5.049107551574707\n",
            "Train loss: 3.246520757675171\n",
            "Train loss: 3.809835433959961\n",
            "Train loss: 4.53020715713501\n",
            "Train loss: 3.668700695037842\n",
            "Train loss: 4.199177265167236\n",
            "Train loss: 4.390470504760742\n",
            "Train loss: 3.8683385848999023\n",
            "Train loss: 3.9282281398773193\n",
            "Train loss: 4.795362949371338\n",
            "Train loss: 4.2298760414123535\n",
            "Train loss: 3.9778053760528564\n",
            "Train loss: 4.64463996887207\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDahBf6IX4py"
      },
      "source": [
        "다음으로 Skip-gram 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGEusqFV5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f00b22d-ecc6-4843-fcbe-34f8f37acf1d"
      },
      "source": [
        "skipgram.train()\n",
        "skipgram = skipgram.to(device)\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(1, num_epochs+1):\n",
        "    print(\"#\" * 50)\n",
        "    print(f\"Epoch: {e}\")\n",
        "    for batch in tqdm(skipgram_loader):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device) # (B, W), (B)\n",
        "        output = skipgram(x)  # (B, V)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss = loss_function(output, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        print(f\"Train loss: {loss.item()}\")\n",
        "\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/64 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.412774085998535\n",
            "Train loss: 4.484925270080566\n",
            "Train loss: 4.81033992767334\n",
            "Train loss: 4.76298189163208\n",
            "Train loss: 3.9532711505889893\n",
            "Train loss: 4.109473705291748\n",
            "Train loss: 3.8395776748657227\n",
            "Train loss: 3.9259257316589355\n",
            "Train loss: 4.001883506774902\n",
            "Train loss: 4.39670467376709\n",
            "Train loss: 4.49104642868042\n",
            "Train loss: 4.329383850097656\n",
            "Train loss: 4.49345064163208\n",
            "Train loss: 3.758206367492676\n",
            "Train loss: 4.382747650146484\n",
            "Train loss: 4.5534892082214355\n",
            "Train loss: 4.2382097244262695\n",
            "Train loss: 4.461404800415039\n",
            "Train loss: 3.818362236022949\n",
            "Train loss: 4.339934825897217\n",
            "Train loss: 4.173030376434326\n",
            "Train loss: 3.7755696773529053\n",
            "Train loss: 4.142513275146484\n",
            "Train loss: 4.019070625305176\n",
            "Train loss: 4.536371231079102\n",
            "Train loss: 4.5141119956970215\n",
            "Train loss: 4.141043186187744\n",
            "Train loss: 4.219099521636963\n",
            "Train loss: 4.730193138122559\n",
            "Train loss: 4.113398551940918\n",
            "Train loss: 4.428716659545898\n",
            "Train loss: 3.928546667098999\n",
            "Train loss: 4.1241230964660645\n",
            "Train loss: 4.387020587921143\n",
            "Train loss: 4.3406548500061035\n",
            "Train loss: 4.388759136199951\n",
            "Train loss: 4.033077716827393\n",
            "Train loss: 4.600700378417969\n",
            "Train loss: 4.466093063354492\n",
            "Train loss: 4.632503509521484\n",
            "Train loss: 4.331130027770996\n",
            "Train loss: 3.694833517074585\n",
            "Train loss: 4.136373519897461\n",
            "Train loss: 4.477707862854004\n",
            "Train loss: 4.214807510375977\n",
            "Train loss: 4.330987453460693\n",
            "Train loss: 3.8630428314208984\n",
            "Train loss: 4.0706634521484375\n",
            "Train loss: 4.461308479309082\n",
            "Train loss: 4.362713813781738\n",
            "Train loss: 4.160404682159424\n",
            "Train loss: 4.393339157104492\n",
            "Train loss: 3.9524402618408203\n",
            "Train loss: 3.915355682373047\n",
            "Train loss: 4.152442455291748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 440.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.269676208496094\n",
            "Train loss: 4.487614631652832\n",
            "Train loss: 4.302535533905029\n",
            "Train loss: 4.677045822143555\n",
            "Train loss: 4.521509170532227\n",
            "Train loss: 3.820033550262451\n",
            "Train loss: 3.936980724334717\n",
            "Train loss: 3.8648953437805176\n",
            "Train loss: 4.656122207641602\n",
            "##################################################\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/64 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.386712074279785\n",
            "Train loss: 4.435376167297363\n",
            "Train loss: 4.780960559844971\n",
            "Train loss: 4.691077709197998\n",
            "Train loss: 3.921077251434326\n",
            "Train loss: 4.070250034332275"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 46/64 [00:00<00:00, 456.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 3.8046278953552246\n",
            "Train loss: 3.896162509918213\n",
            "Train loss: 3.976484537124634\n",
            "Train loss: 4.368732929229736\n",
            "Train loss: 4.461035251617432\n",
            "Train loss: 4.295424461364746\n",
            "Train loss: 4.462738990783691\n",
            "Train loss: 3.725959300994873\n",
            "Train loss: 4.341187477111816\n",
            "Train loss: 4.522125720977783\n",
            "Train loss: 4.209795951843262\n",
            "Train loss: 4.424056529998779\n",
            "Train loss: 3.7872986793518066\n",
            "Train loss: 4.308894157409668\n",
            "Train loss: 4.090639591217041\n",
            "Train loss: 3.689358711242676\n",
            "Train loss: 4.09320592880249\n",
            "Train loss: 3.9913628101348877\n",
            "Train loss: 4.493369102478027\n",
            "Train loss: 4.44356107711792\n",
            "Train loss: 4.09338903427124\n",
            "Train loss: 4.190280437469482\n",
            "Train loss: 4.683515548706055\n",
            "Train loss: 4.0890092849731445\n",
            "Train loss: 4.3978271484375\n",
            "Train loss: 3.8988118171691895\n",
            "Train loss: 4.088562965393066\n",
            "Train loss: 4.35234260559082\n",
            "Train loss: 4.3031005859375\n",
            "Train loss: 4.353830337524414\n",
            "Train loss: 3.9834694862365723\n",
            "Train loss: 4.543308734893799\n",
            "Train loss: 4.436615943908691\n",
            "Train loss: 4.600053787231445\n",
            "Train loss: 4.308138847351074\n",
            "Train loss: 3.6564321517944336\n",
            "Train loss: 4.067763328552246\n",
            "Train loss: 4.42207145690918\n",
            "Train loss: 4.0866899490356445\n",
            "Train loss: 4.248520374298096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 64/64 [00:00<00:00, 336.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3.773509979248047\n",
            "Train loss: 4.02045202255249\n",
            "Train loss: 4.43445348739624\n",
            "Train loss: 4.326536178588867\n",
            "Train loss: 4.1214375495910645\n",
            "Train loss: 4.349700450897217\n",
            "Train loss: 3.9237871170043945\n",
            "Train loss: 3.8951263427734375\n",
            "Train loss: 4.126762866973877\n",
            "Train loss: 4.247681140899658\n",
            "Train loss: 4.419980049133301\n",
            "Train loss: 4.269495010375977\n",
            "Train loss: 4.646003246307373\n",
            "Train loss: 4.484131813049316\n",
            "Train loss: 3.792377233505249\n",
            "Train loss: 3.911505699157715\n",
            "Train loss: 3.8463146686553955\n",
            "Train loss: 4.607560634613037\n",
            "##################################################\n",
            "Epoch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/64 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.361071586608887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 39/64 [00:00<00:00, 387.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.386081218719482\n",
            "Train loss: 4.751664638519287\n",
            "Train loss: 4.619998931884766\n",
            "Train loss: 3.8891243934631348\n",
            "Train loss: 4.031692028045654\n",
            "Train loss: 3.7702202796936035\n",
            "Train loss: 3.8666021823883057\n",
            "Train loss: 3.9512481689453125\n",
            "Train loss: 4.340885162353516\n",
            "Train loss: 4.431173324584961\n",
            "Train loss: 4.26162576675415\n",
            "Train loss: 4.432494163513184\n",
            "Train loss: 3.6939711570739746\n",
            "Train loss: 4.299925327301025\n",
            "Train loss: 4.4910736083984375\n",
            "Train loss: 4.181578636169434\n",
            "Train loss: 4.386899471282959\n",
            "Train loss: 3.7566308975219727\n",
            "Train loss: 4.278003692626953\n",
            "Train loss: 4.009209156036377\n",
            "Train loss: 3.6055493354797363\n",
            "Train loss: 4.0443902015686035\n",
            "Train loss: 3.9638149738311768\n",
            "Train loss: 4.450762748718262\n",
            "Train loss: 4.373871803283691\n",
            "Train loss: 4.04624080657959\n",
            "Train loss: 4.161921501159668\n",
            "Train loss: 4.637198448181152\n",
            "Train loss: 4.06487512588501\n",
            "Train loss: 4.3670783042907715\n",
            "Train loss: 3.8693554401397705\n",
            "Train loss: 4.05327844619751\n",
            "Train loss: 4.318037986755371\n",
            "Train loss: 4.265812873840332\n",
            "Train loss: 4.31913423538208\n",
            "Train loss: 3.9346988201141357\n",
            "Train loss: 4.4878411293029785\n",
            "Train loss: 4.407485485076904\n",
            "Train loss: 4.56790828704834\n",
            "Train loss: 4.2851996421813965\n",
            "Train loss: 3.6187326908111572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 64/64 [00:00<00:00, 410.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.001234531402588\n",
            "Train loss: 4.366674900054932\n",
            "Train loss: 3.9613547325134277\n",
            "Train loss: 4.167032241821289\n",
            "Train loss: 3.6865673065185547\n",
            "Train loss: 3.9707388877868652\n",
            "Train loss: 4.407710075378418\n",
            "Train loss: 4.290859699249268\n",
            "Train loss: 4.083111763000488\n",
            "Train loss: 4.306543350219727\n",
            "Train loss: 3.8953235149383545\n",
            "Train loss: 3.8753232955932617\n",
            "Train loss: 4.101219177246094\n",
            "Train loss: 4.225779056549072\n",
            "Train loss: 4.353176116943359\n",
            "Train loss: 4.236832141876221\n",
            "Train loss: 4.615204334259033\n",
            "Train loss: 4.446969985961914\n",
            "Train loss: 3.764899730682373\n",
            "Train loss: 3.8863701820373535\n",
            "Train loss: 3.8281633853912354\n",
            "Train loss: 4.559430122375488\n",
            "##################################################\n",
            "Epoch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/64 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.335845947265625\n",
            "Train loss: 4.33704948425293\n",
            "Train loss: 4.7224531173706055\n",
            "Train loss: 4.549798011779785\n",
            "Train loss: 3.8574142456054688\n",
            "Train loss: 3.993809700012207\n",
            "Train loss: 3.736358165740967\n",
            "Train loss: 3.8372485637664795\n",
            "Train loss: 3.9261741638183594"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 42/64 [00:00<00:00, 414.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 4.313161849975586\n",
            "Train loss: 4.401461601257324\n",
            "Train loss: 4.227993011474609\n",
            "Train loss: 4.402718544006348\n",
            "Train loss: 3.6622445583343506\n",
            "Train loss: 4.258969306945801\n",
            "Train loss: 4.460338115692139\n",
            "Train loss: 4.153557777404785\n",
            "Train loss: 4.349937438964844\n",
            "Train loss: 3.726362705230713\n",
            "Train loss: 4.247262954711914\n",
            "Train loss: 3.9288182258605957\n",
            "Train loss: 3.524254083633423\n",
            "Train loss: 3.9960803985595703\n",
            "Train loss: 3.9364306926727295\n",
            "Train loss: 4.4085588455200195\n",
            "Train loss: 4.3050994873046875\n",
            "Train loss: 3.9996161460876465\n",
            "Train loss: 4.134019374847412\n",
            "Train loss: 4.591253757476807\n",
            "Train loss: 4.040996074676514\n",
            "Train loss: 4.33647346496582\n",
            "Train loss: 3.840179681777954\n",
            "Train loss: 4.018272876739502\n",
            "Train loss: 4.284112453460693\n",
            "Train loss: 4.228797912597656\n",
            "Train loss: 4.284675121307373\n",
            "Train loss: 3.886821985244751\n",
            "Train loss: 4.434363842010498\n",
            "Train loss: 4.378697395324707\n",
            "Train loss: 4.536068916320801\n",
            "Train loss: 4.262306213378906\n",
            "Train loss: 3.5817484855651855\n",
            "Train loss: 3.9368703365325928\n",
            "Train loss: 4.311528205871582\n",
            "Train loss: 3.8389954566955566\n",
            "Train loss: 4.086607456207275\n",
            "Train loss: 3.6023404598236084\n",
            "Train loss: 3.9215407371520996\n",
            "Train loss: 4.381076812744141\n",
            "Train loss: 4.255688667297363\n",
            "Train loss: 4.045438766479492\n",
            "Train loss: 4.263874053955078"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 64/64 [00:00<00:00, 371.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train loss: 3.8670506477355957\n",
            "Train loss: 3.8559410572052\n",
            "Train loss: 4.075813293457031\n",
            "Train loss: 4.203970909118652\n",
            "Train loss: 4.287261962890625\n",
            "Train loss: 4.204552173614502\n",
            "Train loss: 4.584650039672852\n",
            "Train loss: 4.410028457641602\n",
            "Train loss: 3.737602710723877\n",
            "Train loss: 3.8615732192993164\n",
            "Train loss: 3.810431957244873\n",
            "Train loss: 4.511737823486328\n",
            "##################################################\n",
            "Epoch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/64 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.311031341552734\n",
            "Train loss: 4.288290977478027\n",
            "Train loss: 4.6933274269104\n",
            "Train loss: 4.480535507202148\n",
            "Train loss: 3.8259506225585938\n",
            "Train loss: 3.9566121101379395\n",
            "Train loss: 3.7030467987060547\n",
            "Train loss: 3.8081045150756836\n",
            "Train loss: 3.901261568069458\n",
            "Train loss: 4.285566329956055\n",
            "Train loss: 4.371899604797363\n",
            "Train loss: 4.194527626037598\n",
            "Train loss: 4.3734130859375\n",
            "Train loss: 3.6307830810546875\n",
            "Train loss: 4.218328475952148\n",
            "Train loss: 4.429924011230469\n",
            "Train loss: 4.125732898712158\n",
            "Train loss: 4.313175201416016\n",
            "Train loss: 3.6964962482452393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 571.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 4.216671943664551\n",
            "Train loss: 3.849550485610962\n",
            "Train loss: 3.4455957412719727\n",
            "Train loss: 3.9482946395874023\n",
            "Train loss: 3.9092109203338623\n",
            "Train loss: 4.366767883300781\n",
            "Train loss: 4.237303733825684\n",
            "Train loss: 3.9535322189331055\n",
            "Train loss: 4.106569290161133\n",
            "Train loss: 4.545693397521973\n",
            "Train loss: 4.0173726081848145\n",
            "Train loss: 4.306014060974121\n",
            "Train loss: 3.8112847805023193\n",
            "Train loss: 3.9835495948791504\n",
            "Train loss: 4.250572204589844\n",
            "Train loss: 4.1920623779296875\n",
            "Train loss: 4.250457286834717\n",
            "Train loss: 3.8398993015289307\n",
            "Train loss: 4.3829522132873535\n",
            "Train loss: 4.35024881362915\n",
            "Train loss: 4.504535675048828\n",
            "Train loss: 4.2394514083862305\n",
            "Train loss: 3.5454928874969482\n",
            "Train loss: 3.8747622966766357\n",
            "Train loss: 4.256642818450928\n",
            "Train loss: 3.719822645187378\n",
            "Train loss: 4.007333755493164\n",
            "Train loss: 3.520963430404663\n",
            "Train loss: 3.8728742599487305\n",
            "Train loss: 4.354556560516357\n",
            "Train loss: 4.221024036407471\n",
            "Train loss: 4.008426666259766\n",
            "Train loss: 4.2216997146606445\n",
            "Train loss: 3.838970184326172\n",
            "Train loss: 3.8369717597961426\n",
            "Train loss: 4.050546646118164\n",
            "Train loss: 4.18225622177124\n",
            "Train loss: 4.222295761108398\n",
            "Train loss: 4.172659873962402\n",
            "Train loss: 4.554343223571777\n",
            "Train loss: 4.373310089111328\n",
            "Train loss: 3.7104849815368652\n",
            "Train loss: 3.837113380432129\n",
            "Train loss: 3.7931127548217773\n",
            "Train loss: 4.4644951820373535\n",
            "Finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0sbHV6dEOR"
      },
      "source": [
        "### 1.5 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGarLWxXeJvz"
      },
      "source": [
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1wrl-L_RjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e10af2-fb8f-4e09-a79c-f0b66a9ac548"
      },
      "source": [
        "for word in test_words:\n",
        "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "    emb = cbow.embedding(input_id)\n",
        "\n",
        "    print(f\"Word: {word}\")\n",
        "    print(emb.squeeze(0))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor([ 0.2449, -1.3306,  0.9265, -0.8492, -1.0746, -0.6219,  0.1393,  1.3252,\n",
            "        -0.4218, -0.5153, -0.7044, -0.4300,  0.2803, -1.5810, -0.0364,  0.6941,\n",
            "         2.6620,  0.3427,  0.5508, -0.1912, -0.3278,  0.3970,  0.9122,  0.9230,\n",
            "         1.4270, -0.5188, -1.1903, -0.4654,  0.8021, -0.5123, -1.2608,  2.4610,\n",
            "        -0.8837,  0.9114, -0.5185,  0.1186, -0.6277,  1.5066, -1.4712,  1.4513,\n",
            "        -1.0748,  0.8127,  0.4623, -1.1953,  0.5021, -0.8800,  0.7874,  1.0231,\n",
            "        -0.4771, -0.0560, -0.2483, -0.1670,  0.1708,  0.6910, -1.6905,  0.8516,\n",
            "        -1.2945, -0.7784, -0.0921, -0.2643, -1.2518, -0.2939,  0.8031, -0.7087,\n",
            "        -0.4606,  0.2068,  1.7111,  0.0448,  0.0709,  2.3057,  0.4375, -0.1735,\n",
            "        -0.6394, -0.4977, -0.5806,  0.5690, -0.9318,  1.4501,  0.6759,  0.4802,\n",
            "         0.2585, -1.2619,  0.9885, -0.4609,  0.6667,  0.3961, -0.3991, -0.8461,\n",
            "         1.1024, -0.9006, -0.2356, -0.1737, -2.2562,  0.2946, -1.7781,  1.2225,\n",
            "         1.5176,  0.2725, -0.0668,  0.5029,  0.1765,  0.4184, -1.2719,  0.5245,\n",
            "         0.8383,  0.0068,  0.5172,  2.2596,  1.6601,  0.8577, -0.0922, -1.3997,\n",
            "        -0.1538,  0.3881,  0.6596,  0.1864,  0.5778, -0.4677, -0.1762, -1.2257,\n",
            "         0.9216,  0.5318, -0.2476,  0.0054,  0.5448,  0.4609,  0.9070, -0.1860,\n",
            "        -0.9500,  0.0487, -0.1700,  0.5434, -0.6640,  0.6974, -0.5273,  0.5965,\n",
            "        -1.6883,  1.7022,  0.4079, -0.0542, -0.7031, -0.7344, -0.8197, -1.3932,\n",
            "         0.8928,  0.0798,  0.0625,  0.0545, -0.1244,  0.8326,  2.3009, -0.1350,\n",
            "        -0.8462, -0.2234, -1.3128, -0.6444, -0.7919,  0.3309, -0.1076,  1.1998,\n",
            "        -0.3678, -0.4325,  1.2325, -0.4712,  0.3865,  1.4946,  0.2815, -1.9743,\n",
            "        -1.5522, -0.4371, -0.5790, -0.2113, -1.6208, -0.2875, -0.9731, -0.9045,\n",
            "         0.0786,  1.7308, -0.5381, -0.4335,  1.3246, -2.1616, -0.9294,  0.9756,\n",
            "        -0.2359, -0.7378, -1.0844, -0.8171,  0.9972,  0.8824, -0.7825, -1.2053,\n",
            "        -1.1104,  0.6551,  0.0518, -0.4466, -0.2750, -0.7016, -0.5234, -0.3088,\n",
            "         1.5511,  0.5481, -0.2642,  0.6409, -1.3220,  0.7806, -2.0110, -1.5481,\n",
            "        -0.5990, -0.9197,  0.0348,  1.5044, -0.2954, -0.2025, -1.0177, -0.2042,\n",
            "         1.2089,  1.1028, -2.9830,  0.6226,  0.1475, -0.8563,  0.9772,  0.6567,\n",
            "         1.7686,  0.8931, -1.1167, -0.3063, -1.7373,  2.7291, -0.9845, -0.5285,\n",
            "         0.3343,  0.8872, -0.5110, -3.5686, -2.1707, -0.3008,  0.7128,  0.5772,\n",
            "         1.4212,  0.7056,  1.4033, -0.8412,  0.1717, -0.2496, -0.6959,  1.2962,\n",
            "         0.1576, -0.7379,  1.0113,  0.4014, -1.1540, -0.7297, -0.0233, -1.2163],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "Word: 맛\n",
            "tensor([-0.5225, -0.5784,  1.5418,  1.5434,  0.8593, -0.8706,  0.0033, -0.7005,\n",
            "        -0.4459, -0.2788,  1.2609, -0.9551,  1.4185, -0.3730, -0.4799, -0.2107,\n",
            "        -0.4886,  1.1160,  0.5573,  1.0094, -0.1054, -0.0122, -0.0810,  0.2070,\n",
            "        -1.9127,  1.0776, -0.2160,  2.5222, -0.8534, -1.8216, -0.0161, -0.5900,\n",
            "         0.8528,  0.1757,  0.2919,  0.4725, -1.6305, -0.8655, -0.4758,  0.6880,\n",
            "        -0.2461,  0.1716,  1.2691,  0.8451,  0.6424, -0.9034, -1.1576, -1.0427,\n",
            "         0.7685, -0.5655, -0.9574,  0.0230, -2.9148, -1.4801, -0.7034,  1.6197,\n",
            "        -0.5196, -0.6381,  2.1204, -0.1746, -0.0248,  0.1017, -0.6533,  0.9019,\n",
            "         0.5215, -0.9853,  2.1105,  0.7161, -1.3890, -0.1842,  1.9803,  0.2574,\n",
            "        -1.1115, -0.8060, -0.2375,  0.8574, -0.2986,  2.1264, -0.2517, -0.0675,\n",
            "         0.4358, -1.2429, -0.4906,  0.4853, -0.9349,  0.1508, -1.1847, -1.6570,\n",
            "         0.7693,  0.0498,  0.0783,  0.1935, -0.7100,  0.7965, -0.9871, -0.5782,\n",
            "        -1.0136,  0.1713,  0.3797,  1.1097, -0.8171, -0.3840,  0.4126, -0.5266,\n",
            "        -1.8381, -2.1597, -1.1062, -0.5029,  0.4901,  0.4160,  1.5140,  0.0649,\n",
            "        -0.6652,  0.4036,  0.1037,  0.2997, -0.0489, -0.9732,  1.7350, -0.6387,\n",
            "         2.4130,  0.2374, -0.2690, -2.0273,  1.2313,  0.4547, -0.0458,  0.2955,\n",
            "        -1.3929,  1.1100,  0.7855,  0.1482,  0.8921, -1.7431,  0.4185, -1.8840,\n",
            "        -0.8585, -0.7769,  0.2396, -0.3286,  0.1895, -0.0980, -1.9997, -1.0752,\n",
            "        -0.0129, -0.4206,  0.2179, -0.3067, -1.5342,  0.2137,  0.1864, -0.2310,\n",
            "        -0.5016, -1.6915, -0.1863, -0.7809,  1.1368, -1.1834,  1.6881, -0.5740,\n",
            "         1.5852,  0.6313,  1.6166, -0.9187,  1.1552,  0.7198,  0.5126, -0.8579,\n",
            "        -0.7979,  0.0865, -1.0612, -0.3372, -0.6924,  0.8785,  0.2294, -0.4172,\n",
            "        -0.0104,  0.2474,  0.1122, -0.6454, -0.8387, -0.8241, -1.1996,  0.3245,\n",
            "         2.1613,  1.2261,  2.1380,  1.2286,  0.5076,  0.2779,  0.5141,  0.4742,\n",
            "        -1.5660,  1.2885, -0.1998,  0.6435,  1.0685, -1.3630,  0.6743, -0.5326,\n",
            "         0.0670,  0.3446, -0.2336,  0.2858, -0.6474, -1.2653,  0.1300,  0.4559,\n",
            "         0.3731, -2.4378,  0.1307, -0.2577,  0.2028, -1.9348,  0.0243, -0.0953,\n",
            "         1.0262,  0.4086, -0.5626, -0.5207, -1.7651,  1.0077, -0.1931,  0.3587,\n",
            "         2.1179, -0.6039, -1.1680,  1.5584,  0.7537, -1.6440, -2.5541,  2.0767,\n",
            "        -0.1364, -0.9388, -0.7047,  0.0426,  2.3685,  0.0081,  0.4711,  0.9041,\n",
            "        -0.8926, -0.8729, -0.6255, -0.4805,  0.6690,  0.4554,  0.5928,  0.3598,\n",
            "         1.0218,  0.6017, -0.5737, -0.6068, -0.0975,  2.2881,  0.1194,  0.3387],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "Word: 서비스\n",
            "tensor([-0.3237,  0.2549, -0.0600,  0.8527, -0.1752,  1.1164,  0.5186, -0.2717,\n",
            "         1.4750,  0.0310, -0.2372,  1.1015, -0.2554,  0.6535, -0.8254, -0.1762,\n",
            "         2.2859, -1.0449, -0.3621,  0.1865,  0.3293, -0.9866, -1.0131,  1.1424,\n",
            "         0.3663,  0.0693, -0.5505, -0.7113, -0.5292, -1.0125, -0.4991, -0.0671,\n",
            "        -0.3573,  0.1562, -1.1734,  2.7975, -0.7453,  0.7700, -1.0788, -2.2291,\n",
            "         1.2530,  0.5679, -0.8911, -0.9075, -1.4626, -0.4642,  0.5264,  1.3873,\n",
            "         0.1663,  0.6971,  0.7712, -1.0912, -1.0167,  0.0151, -0.7920, -0.0061,\n",
            "         0.5100,  1.2885,  0.1541, -0.1108,  1.0454, -0.5175, -1.2844, -0.6490,\n",
            "        -1.0758,  1.6170,  0.5556,  1.1730, -0.8820, -0.1260,  0.0263,  2.0237,\n",
            "        -0.8920,  1.0376, -0.6103, -0.2689,  0.6592,  0.4002, -0.7010, -1.4452,\n",
            "         1.0603, -1.1038,  0.3450, -0.4660,  0.3734,  0.9447, -0.6324, -0.7396,\n",
            "        -0.0510, -0.3009, -1.5815, -0.8240, -0.3900, -0.4364,  0.3454, -0.6682,\n",
            "         1.2050,  0.0888, -0.3285,  0.4481,  1.5709,  1.2766,  2.4081,  0.3240,\n",
            "         1.0817,  0.8977, -0.9409, -0.4138, -0.4353,  1.4600, -0.2351,  0.5704,\n",
            "         0.1133, -1.6078,  0.6608, -0.0165, -1.4608,  1.2293,  1.7503,  0.5690,\n",
            "        -0.1105,  1.2678,  1.4773, -1.1824, -0.8791, -1.4685, -0.6833, -0.8088,\n",
            "        -0.0682, -0.8495,  0.8785,  0.6485, -1.6686,  0.0591,  0.2358, -0.3969,\n",
            "         1.8563,  0.4872, -0.3878, -0.9352, -1.2069,  0.9016,  0.0780, -0.9555,\n",
            "        -0.5242,  1.1281,  0.4557, -0.6108,  0.4295, -0.6306,  1.0599, -1.1883,\n",
            "        -1.1349, -2.1058,  1.2622, -1.7106, -0.1478,  0.2828, -0.2850, -0.6092,\n",
            "         0.2703, -0.9344, -1.2451, -1.4914,  1.5480, -1.2167, -0.7065, -0.3271,\n",
            "         0.6095,  1.5670, -0.8633,  0.4171,  1.6751,  0.8777,  0.3955, -1.4747,\n",
            "        -0.6414, -0.2312,  1.3281,  0.0684, -0.9711,  0.4982,  1.4592, -0.4417,\n",
            "         0.0918,  0.1290, -0.1423,  1.3240,  0.8193, -0.6936,  0.9892, -1.4608,\n",
            "         1.0630,  1.1575,  1.2373,  1.2329, -0.0852, -0.1899,  0.2313,  0.8881,\n",
            "        -1.6429, -0.2842,  0.7953, -0.9964, -0.2089,  0.3147, -0.5157,  0.5425,\n",
            "         1.7337, -0.0743,  0.2130, -0.2949,  0.2310,  0.2406,  0.6423, -1.6290,\n",
            "         0.5465, -0.5648, -1.6219, -1.6123,  1.7869,  0.0076, -0.7116,  1.0536,\n",
            "        -0.5492, -1.6834, -0.7996, -1.3853,  0.9378, -1.4556, -0.1519, -0.9333,\n",
            "         0.2346,  1.8196,  0.6819, -0.1881,  0.8449,  1.1807,  0.0728, -0.5736,\n",
            "        -2.3587, -0.3342, -1.5265, -0.3301,  0.7183,  0.8776,  0.5515,  0.5492,\n",
            "        -0.5918,  1.0872,  0.7004,  0.3187, -0.4013, -0.3770,  0.4846, -0.6476],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "Word: 위생\n",
            "tensor([ 0.0587, -0.5124, -0.6399, -0.0377, -0.8392,  1.0336, -0.8251,  0.6338,\n",
            "        -0.7914, -0.2740, -0.2421, -1.4369, -0.6872, -0.6766,  0.1219,  0.9733,\n",
            "        -0.9971,  0.0871,  0.7821,  0.8491,  0.5171,  1.2303,  0.7227,  0.1963,\n",
            "         0.3472,  1.8761, -1.1443,  0.8249, -1.2927,  2.0976, -0.1105, -0.2464,\n",
            "        -1.1944, -0.0401,  1.1265, -1.0841,  1.7692,  0.0047,  1.5973,  1.6196,\n",
            "         0.5525, -1.4532, -0.2939, -1.1450, -0.6557, -0.4097,  0.0784, -0.1005,\n",
            "         0.1676, -0.5015, -1.1971,  1.9806, -2.4569,  0.3580,  0.1387,  0.6753,\n",
            "         0.0633, -0.1468,  0.3765, -1.4799,  1.2532,  0.8654, -0.6082, -0.5808,\n",
            "         0.7417,  1.4089, -1.5004,  1.3506,  0.9463,  0.7921, -0.8896,  0.7338,\n",
            "         0.3755,  0.9061,  0.0840,  0.6044, -1.3065, -0.4165,  0.3357, -1.1342,\n",
            "        -1.2943,  2.2398, -1.1352,  0.0288, -0.8326,  0.1409,  0.8709,  1.3722,\n",
            "        -0.9129,  2.0415,  0.4767, -0.6335,  0.0039,  1.0211,  0.0959,  0.0382,\n",
            "         0.4657, -0.6638, -0.5981,  0.0082,  0.3031,  0.4139, -0.2973, -0.4833,\n",
            "        -0.6628,  0.0850, -0.4510,  0.6064, -0.1600,  0.1421,  0.5679,  0.3713,\n",
            "         0.6791, -0.5200,  0.8655,  1.5555, -0.4836,  0.1286, -1.4580, -0.6588,\n",
            "        -0.5581,  0.3183, -0.5454, -1.9945,  1.9808,  1.3615, -0.3152,  0.0070,\n",
            "         0.5590, -0.4213,  1.0688, -0.4495, -0.0866, -0.3566,  0.4211,  0.9321,\n",
            "         0.6596, -0.2298,  1.4645,  0.6607,  0.1245, -0.5160, -0.3127,  0.8163,\n",
            "         0.6856,  0.2529, -0.1480, -0.0445, -0.9569,  0.4034,  0.5184, -2.9856,\n",
            "         2.0545, -0.8560,  0.4549, -0.4454, -0.7638,  1.1526, -1.7594, -0.1907,\n",
            "        -0.3612,  0.4221, -0.2909, -1.0545,  0.0800,  0.2507, -1.9840, -0.6478,\n",
            "        -0.5808,  0.3800,  0.3989,  0.6389, -0.2001, -1.3491,  1.2468, -1.1921,\n",
            "        -0.4378,  1.1897, -0.1991,  1.5036,  1.8000, -1.8142, -0.4977,  0.4136,\n",
            "        -0.0277,  2.8441, -0.0825,  0.4769, -1.1419,  1.2760, -1.1590,  0.9586,\n",
            "         0.7550, -2.0644,  0.5490,  1.2769,  1.1229, -0.5925, -2.4231,  0.4867,\n",
            "        -0.7193, -1.3567,  1.0876, -1.9982, -1.1801,  0.3015,  0.0482, -1.4122,\n",
            "        -1.1786, -0.2850,  1.0450, -0.6594,  0.0387,  0.2824,  1.0897, -0.6042,\n",
            "        -0.2706,  0.4808, -1.8958,  0.8610, -2.3883, -0.5457, -0.1256, -1.3288,\n",
            "        -1.5822, -0.4502,  0.7972, -0.6666,  0.4805, -0.6335,  0.1389,  1.1056,\n",
            "         1.2396,  0.3909,  0.1834, -0.9447,  1.7693,  1.5564,  1.0112, -1.1497,\n",
            "         0.9073, -0.2809, -0.8226,  1.4306,  1.1661,  0.7145,  1.2090,  1.5063,\n",
            "         0.0309,  0.7825,  0.4164, -0.5719,  0.9639, -0.5380, -0.6594,  0.7558],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "Word: 가격\n",
            "tensor([-1.2262, -1.1032,  1.5680, -0.7952,  0.0098,  0.0389, -2.5409,  0.9543,\n",
            "         0.4657, -1.2651, -0.7685, -3.4934,  0.4100, -0.6050,  0.0218, -1.6478,\n",
            "        -2.1027, -0.2781,  1.5579,  0.1713, -0.4430, -0.9782, -0.1539, -0.9814,\n",
            "         0.7355, -1.2719,  1.0010, -0.1093,  0.7204, -0.8934,  0.4509, -0.3626,\n",
            "        -0.3479,  0.6289, -0.3756, -0.4822, -0.8331, -0.2182, -1.4984, -0.0581,\n",
            "        -0.1225,  0.7896, -0.2815, -0.7650,  0.0985,  1.3019, -0.2525, -0.9506,\n",
            "        -0.0905, -1.6359, -0.0252,  1.9717, -0.7924,  0.3068,  0.1418,  0.1096,\n",
            "        -0.3172, -1.0542,  1.1988,  0.4907, -0.4327, -1.0072, -0.7536, -0.6465,\n",
            "         1.6312,  0.2283, -0.7025,  0.1565,  2.1621, -1.2342,  0.0508, -0.8851,\n",
            "         1.9755,  0.1252, -0.2483,  0.2489, -0.2583,  0.8350,  0.3182, -2.3996,\n",
            "        -0.6259,  0.2390, -1.3757, -0.3046,  0.9657, -1.4154, -0.5005,  1.4364,\n",
            "         0.8442,  0.6300, -2.1916,  0.5826, -0.7436, -0.2995,  1.7422, -0.1516,\n",
            "         0.1634, -1.1007,  2.1195, -1.4801,  0.6053,  0.4871,  1.4199, -0.5105,\n",
            "        -0.7753,  0.2084, -1.0089,  0.8376,  0.5548,  2.3303, -1.0706,  0.1002,\n",
            "         1.3778,  0.3762,  0.5853,  1.3309,  0.9252,  0.8275,  0.1126, -0.2750,\n",
            "         0.5955,  0.0131,  2.1996, -1.0151, -1.1787,  0.7906,  0.8081, -1.8271,\n",
            "        -0.7069, -0.1431, -0.0242, -2.8743, -2.0001, -0.5526,  2.8682, -0.1172,\n",
            "        -1.3719,  2.3975, -0.2275,  0.5083, -1.3441, -0.3247, -1.9158,  0.3273,\n",
            "         0.6768, -0.7771, -1.9092, -0.1322, -0.8089, -0.4844, -0.5273,  2.2608,\n",
            "        -0.6875,  1.7645,  1.7585, -1.4253, -0.6702, -0.6510, -1.8459, -0.6841,\n",
            "         0.1857, -0.9970,  1.2035, -0.8267,  0.0049, -1.3900, -0.3040, -0.9268,\n",
            "        -0.9309, -1.7190,  0.4603, -1.1773, -0.4852,  0.9325, -0.8332, -0.8815,\n",
            "         0.5569, -1.3366,  0.9159,  0.1806, -1.7661,  0.3740,  0.5949,  0.3926,\n",
            "        -0.0545,  0.1685, -0.0632,  0.1875,  1.2192, -0.3304,  0.2844,  0.0599,\n",
            "        -0.8827,  0.3631, -1.8083, -0.5567, -0.3098, -0.1453, -0.1780, -0.1769,\n",
            "        -1.2042, -0.1458,  0.1499,  0.0304,  1.4233,  0.8677, -0.0991,  1.1767,\n",
            "         0.1043,  0.1696,  0.6815,  0.9468,  0.4616, -0.4886,  0.4798,  1.1592,\n",
            "        -1.8645,  0.2203,  1.3256, -0.1626, -0.1970, -0.7761,  0.1409, -1.4736,\n",
            "         1.4022,  0.9493, -0.0684,  0.9883,  0.6050, -1.8096, -0.8278, -2.1424,\n",
            "         0.1560, -1.6173,  0.4015, -1.7557, -1.1350,  0.2058,  1.1808,  1.1534,\n",
            "         1.5055, -2.4195,  0.4670, -1.2612, -0.9184,  0.2161,  0.4553, -0.4226,\n",
            "        -0.4345, -0.8614,  0.9620,  0.2910,  1.4736,  1.5026, -1.2778, -2.0310],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5cPRZZe-R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333431a2-2944-44f3-b91d-bbe96d7b8f64"
      },
      "source": [
        "for word in test_words:\n",
        "    input_id = torch.LongTensor([w2i[word]]).to(device)\n",
        "    emb = skipgram.embedding(input_id)\n",
        "\n",
        "    print(f\"Word: {word}\")\n",
        "    print(max(emb.squeeze(0)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: 음식\n",
            "tensor(3.1553, grad_fn=<UnbindBackward0>)\n",
            "Word: 맛\n",
            "tensor(2.7224, grad_fn=<UnbindBackward0>)\n",
            "Word: 서비스\n",
            "tensor(2.3840, grad_fn=<UnbindBackward0>)\n",
            "Word: 위생\n",
            "tensor(2.6414, grad_fn=<UnbindBackward0>)\n",
            "Word: 가격\n",
            "tensor(2.7044, grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elr0HDUs0PVV"
      },
      "source": [
        "## **2. gensim library 를 활용한 word2vec 학습 및 시각화**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzLuaoBh0gjO"
      },
      "source": [
        "### 2.1 필요한 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmkyRxkBo2te"
      },
      "source": [
        "# To ignore deprecated warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from gensim.models import word2vec #  word2vec 관련 모델을 제공해주는 library  \n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews # corpus 를 가져오는 용도 \n",
        "from sklearn.manifold import TSNE # 시각화를 위한 TSNE 사용 \n",
        "from sklearn.metrics import accuracy_score # accuracy score 를 계산하기 위한 metric 라이브러리 불러오기 \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "from torch.utils import data # pytorch data class \n",
        "import torch.nn as nn # pytorch neural network 불러오기 \n",
        "import torch.nn.utils.rnn as rnn_utils # rnn utils\n",
        "import torch\n",
        "\n",
        "import pickle\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQy-fC8q0kqO"
      },
      "source": [
        "### 2.2 Word embedding 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jxAvXU50eKc"
      },
      "source": [
        "# 파라메터값 지정\n",
        "num_features = 300 # 임베딩 벡터 사이즈\n",
        "negative = 10 # negative sampling할 단어 수\n",
        "min_word_count = 10 # 한 문장에 대한 최소 문자 수\n",
        "window = 5 # context window 사이즈\n",
        "downsampling = 0.75 # 단어의 빈도수가 높은 단어에 대해 빈도수를 낮춤\n",
        "epoch = 5 # epoch 수"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLs117S_0q31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ca1fa01a-37aa-4c1d-f52a-a2ba802f69c9"
      },
      "source": [
        "# preparing data\n",
        "sentences = []\n",
        "pos_data = open(\"./data/pos_train.txt\").readlines()\n",
        "neg_data = open(\"./data/neg_train.txt\").readlines()\n",
        "data_ = pos_data + neg_data\n",
        "for line in data_:\n",
        "    sentences.append(line.strip().split(' '))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-15d5fc31bf6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# preparing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpos_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/pos_train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mneg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/neg_train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/pos_train.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnEE3TeT0tSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "384a8e33-2f76-42a4-ad1d-ff2baf59b0bb"
      },
      "source": [
        "# skip-gram 모델 학습\n",
        "skip_gram = word2vec.Word2Vec(sentences,\n",
        "                              sg = 1, # skip-gram\n",
        "                              negative=negative,\n",
        "                              size=num_features, \n",
        "                              min_count=min_word_count,\n",
        "                              window=window,\n",
        "                              sample=downsampling,\n",
        "                              iter=epoch)\n",
        "\n",
        "# CBOW 모델 학습\n",
        "CBOW = word2vec.Word2Vec(sentences,\n",
        "                              sg = 0, # CBOW\n",
        "                              negative=negative,\n",
        "                              size=num_features, \n",
        "                              min_count=min_word_count,\n",
        "                              window=window,\n",
        "                              sample=downsampling,\n",
        "                              iter=epoch)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-12-29 02:47:39,840 : INFO : collecting all words and their counts\n",
            "2021-12-29 02:47:39,842 : INFO : collected 0 word types from a corpus of 0 raw words and 0 sentences\n",
            "2021-12-29 02:47:39,844 : INFO : Loading a fresh vocabulary\n",
            "2021-12-29 02:47:39,845 : INFO : effective_min_count=10 retains 0 unique words (0% of original 0, drops 0)\n",
            "2021-12-29 02:47:39,850 : INFO : effective_min_count=10 leaves 0 word corpus (0% of original 0, drops 0)\n",
            "2021-12-29 02:47:39,852 : INFO : deleting the raw counts dictionary of 0 items\n",
            "2021-12-29 02:47:39,855 : INFO : sample=0.75 downsamples 0 most-common words\n",
            "2021-12-29 02:47:39,856 : INFO : downsampling leaves estimated 0 word corpus (0.0% of prior 0)\n",
            "2021-12-29 02:47:39,858 : INFO : estimated required memory for 0 words and 300 dimensions: 0 bytes\n",
            "2021-12-29 02:47:39,859 : INFO : resetting layer weights\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-04c8349186a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                               \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                               \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownsampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                               iter=epoch)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# CBOW 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAXe5FTe0wZE"
      },
      "source": [
        "### 2.3 학습한 embedding 확인 및 시각화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXhwA4ef1Jfg"
      },
      "source": [
        "특정 단어와 유사한 의미를 가지는 단어를 추출해 봄으로써 임베딩에 단어의 의미가 잘 학습되었는지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8PKSyb30t3y"
      },
      "source": [
        "skip_gram.wv.most_similar(\"man\") # skip-gram 모델을 이용하여 'man'에 가장 유사한 단어 추출"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp8t9M981FLd"
      },
      "source": [
        "CBOW.wv.most_similar(\"man\") # CBOW 모델을 이용하여 'man'에 가장 유사한 단어 추출"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY63knKC1VvK"
      },
      "source": [
        "300차원의 단어 임베딩을 2차원으로 차원 축소하여 시각화함으로써 임베딩 벡터의 분포를 확인해 봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyhhiaUB3sZP"
      },
      "source": [
        "**Tip** `t-SNE` \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "t-SNE 는 차원 축소 (dimensionality reduction)와 시각화(visualization) 방법입니다.\n",
        "\n",
        "Stochastic Neighboring Embedding(SNE) 라는 고차원 공간에 존재하는 데이터 $x$의 이웃 간의 거리를 최대한 보존하는 저차원의 $y$를 학습하는 방법입니다.\n",
        "\n",
        "고차원 공간에서 특정 개체 $x_i$ 가 주어졌을 때 이웃 객체 $x_j$ 가 선택될 확률을 $p_{j|i}$, 저차원 공간에서 특정 개체 $y_i$ 가 주어졌을 때 이웃 객체 $y_j$ 가 선택될 확률을 $q_{j|i}$ 라고 한다면 SNE 는 $p$ 와 $q$의 분포 차이를 최대한 작게 함으로써 학습됩니다. \n",
        "\n",
        "두 확률분포의 유사도는 Kullback-Leibler divergence 라는 지표를 활용합니다.\n",
        "\n",
        "SNE 는 가우시안 분포를 전제하는데, 가우시안 분포가 아닌 t분포를 가정하는 것이 t-SNE입니다.\n",
        "\n",
        "출처: https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLll_Nwh1Gxw"
      },
      "source": [
        "def render_TSNE(vocab, word_emb):\n",
        "    \"\"\"\n",
        "    TSNE를 이용한 word2vec 시각화 \n",
        "    args:\n",
        "        vocab    - vocab list\n",
        "        word_emb - word embeddings\n",
        "    \"\"\"\n",
        "    tsne = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32) #TSNE 시각화를 위한 initialization\n",
        "    _tsne = tsne.fit_transform(word_emb) # TSNE 시각화 적용\n",
        "    x_coordinate = _tsne[:,0] # x 좌표\n",
        "    y_coordinate = _tsne[:,1] # y 좌표\n",
        "\n",
        "    # scatter plot initialization\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(40, 20)\n",
        "    ax.scatter(x_coordinate, y_coordinate)\n",
        "\n",
        "    for i, word in enumerate(random_vocab):\n",
        "        ax.annotate(word,(x_coordinate[i], y_coordinate[i]), fontsize=30) # 각 scatter들에대해 단어 labeling\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I13jgsvI1chr"
      },
      "source": [
        "# Skip-gram 시각화(TSNE)\n",
        "vocab = list(skip_gram.wv.vocab) # vocab list 불러오기\n",
        "random_vocab = random.sample(vocab,k=100) #100개의 임의 단어를 랜덤 샘플링\n",
        "word_emb = skip_gram[random_vocab] # 샘플링된 단어에 대해 학습된 임베딩 벡터 불러오기\n",
        "render_TSNE(random_vocab, word_emb) # TSNE 시각화"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuToMqAs1koT"
      },
      "source": [
        "# CBOW 시각화(TSNE)\n",
        "vocab = list(CBOW.wv.vocab) # vocab list 불러오기\n",
        "random_vocab = random.sample(vocab,k=100) #100개의 임의 단어를 랜덤 샘플링\n",
        "word_emb = skip_gram[random_vocab] # 샘플링된 단어에 대해 학습된 임베딩 벡터 불러오기\n",
        "render_TSNE(random_vocab, word_emb) # TSNE 시각화"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}